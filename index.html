<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Veritas ‚Äî AI Text Detection</title>
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/components.css">
    <link rel="stylesheet" href="css/animations.css">
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;500;600;700&family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!-- KaTeX for mathematical typesetting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
</head>
<body>
    <div class="app-container">
        <!-- Header -->
        <header class="header">
            <div class="logo">
                <span class="logo-icon">‚óà</span>
                <span class="logo-text">VERITAS</span>
                <span class="logo-tagline">AI Text Detection</span>
            </div>
            <nav class="nav">
                <a href="#" class="nav-link active" data-view="analyze">Analyze</a>
                <a href="#" class="nav-link" data-view="history">History</a>
                <a href="#" class="nav-link" data-view="methodology">Methodology</a>
                <a href="#" class="nav-link" data-view="about">About</a>
            </nav>
            <div class="header-actions">
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <span class="theme-label light-label">Light</span>
                    <span class="theme-label dark-label">Dark</span>
                </button>
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <!-- Analyze View -->
            <section class="view active" id="analyzeView">
                <div class="analyze-container">
                    <!-- Input Panel -->
                    <div class="panel input-panel">
                        <div class="panel-header">
                            <h2>Input Text</h2>
                            <div class="panel-actions">
                                <button class="btn btn-sm btn-ghost" id="sampleBtn">Sample</button>
                                <button class="btn btn-sm btn-ghost" id="clearBtn">Clear</button>
                                <div class="upload-dropdown">
                                    <button class="btn btn-sm btn-ghost" id="uploadDropdownBtn">
                                        Upload ‚ñæ
                                    </button>
                                    <div class="upload-menu" id="uploadMenu">
                                        <label class="upload-option" data-type="txt">
                                            <span class="material-icons upload-icon">description</span> Text File (.txt, .md)
                                            <input type="file" accept=".txt,.md" hidden>
                                        </label>
                                        <label class="upload-option" data-type="docx">
                                            <span class="material-icons upload-icon">article</span> Word Document (.docx)
                                            <input type="file" accept=".docx" hidden>
                                        </label>
                                        <label class="upload-option" data-type="pdf">
                                            <span class="material-icons upload-icon">picture_as_pdf</span> PDF Document (.pdf)
                                            <input type="file" accept=".pdf" hidden>
                                        </label>
                                        <button class="upload-option" id="pasteClipboardBtn" data-type="clipboard">
                                            <span class="material-icons upload-icon">content_paste</span> Paste from Clipboard
                                        </button>
                                        <button class="upload-option" id="googleDocsBtn" data-type="gdocs">
                                            <span class="material-icons upload-icon">article</span> Google Docs URL
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Metadata indicators -->
                        <div class="metadata-bar" id="metadataBar" hidden>
                            <span class="metadata-source" id="sourceIndicator"></span>
                            <span class="metadata-warnings" id="metadataWarnings"></span>
                        </div>
                        
                        <div class="textarea-wrapper">
                            <textarea 
                                id="textInput" 
                                placeholder="Paste or type text to analyze for AI-generated content... (minimum 10 words)

Supports: Plain text, Word documents (.docx), PDFs, and Google Docs URLs"
                                spellcheck="false"
                            ></textarea>
                            <div class="textarea-stats">
                                <span id="charCount">0 characters</span>
                                <span id="wordCount">0 words</span>
                                <span id="sentenceCount">0 sentences</span>
                            </div>
                        </div>
                        <div class="analyze-actions">
                            <button class="btn btn-primary btn-lg" id="analyzeBtn" disabled>
                                <span class="btn-text">Analyze Text</span>
                                <span class="btn-spinner"></span>
                            </button>
                        </div>
                    </div>

                    <!-- Results Panel -->
                    <div class="panel results-panel">
                        <div class="results-placeholder">
                            <div class="placeholder-icon">‚óá</div>
                            <p>Enter text and click "Analyze" to begin detection</p>
                        </div>

                        <!-- Results Content -->
                        <div class="results-content">
                            <!-- Overall Score -->
                            <div class="score-card">
                                <div class="score-header">
                                    <h3>Detection Result</h3>
                                    <div class="text-stats"></div>
                                </div>
                                <div class="score-display">
                                    <div class="overall-score">
                                        <div class="score-ring-container"></div>
                                        <div class="verdict">
                                            <span class="verdict-label"></span>
                                            <span class="verdict-description"></span>
                                        </div>
                                        <div class="confidence">
                                            <span class="confidence-label">Confidence:</span>
                                            <span class="confidence-value">-</span>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <!-- Key Findings -->
                            <div class="findings-card">
                                <h4>Key Findings</h4>
                                <div id="findingsList" class="findings-container"></div>
                            </div>

                            <!-- Results Tabs -->
                            <div class="results-tabs">
                                <div class="tabs" role="tablist">
                                    <button class="tab-btn active" data-tab="tab-highlighted" role="tab" aria-selected="true">Highlighted Text</button>
                                    <button class="tab-btn" data-tab="tab-features" role="tab" aria-selected="false">Feature Analysis</button>
                                    <button class="tab-btn" data-tab="tab-statistics" role="tab" aria-selected="false">Statistics</button>
                                    <button class="tab-btn" data-tab="tab-graphs" role="tab" aria-selected="false">Probability Graphs</button>
                                    <button class="tab-btn" data-tab="tab-report" role="tab" aria-selected="false">Detailed Report</button>
                                </div>

                                <!-- Tab Panels -->
                                <div class="tab-panel active" id="tab-highlighted" role="tabpanel">
                                    <div id="highlightedText" class="highlighted-text-wrapper"></div>
                                </div>

                                <div class="tab-panel" id="tab-features" role="tabpanel" hidden>
                                    <div id="featureAnalysis" class="features-container"></div>
                                </div>

                                <div class="tab-panel" id="tab-statistics" role="tabpanel" hidden>
                                    <div id="advancedStatistics" class="statistics-container"></div>
                                </div>

                                <div class="tab-panel" id="tab-graphs" role="tabpanel" hidden>
                                    <div id="probabilityGraphs" class="graphs-container"></div>
                                    <div id="advancedGraphs" class="graphs-container advanced-graphs">
                                        <div class="graph-section">
                                            <h4>Sentence Length Distribution</h4>
                                            <div id="sentenceLengthHistogram" class="graph-wrapper"></div>
                                        </div>
                                        <div class="graph-section">
                                            <h4>N-gram Repetition Heatmap</h4>
                                            <div id="ngramHeatmap" class="graph-wrapper"></div>
                                        </div>
                                        <div class="graph-section">
                                            <h4>Tone Stability Timeline</h4>
                                            <div id="toneTimeline" class="graph-wrapper"></div>
                                        </div>
                                        <div class="graph-section">
                                            <h4>Word Frequency Distribution</h4>
                                            <div id="zipfChart" class="graph-wrapper"></div>
                                        </div>
                                        <div class="graph-section">
                                            <h4>Feature Contributions</h4>
                                            <div id="featureContributionChart" class="graph-wrapper"></div>
                                        </div>
                                    </div>
                                </div>

                                <div class="tab-panel" id="tab-report" role="tabpanel" hidden>
                                    <div id="detailedReport" class="report-container"></div>
                                </div>
                            </div>

                            <!-- Export Actions -->
                            <div class="export-actions">
                                <button class="btn btn-sm btn-ghost" id="copyResultsBtn">Copy Results</button>
                                <button class="btn btn-sm btn-primary" id="viewFullReportBtn">
                                    View Full Report
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- History View -->
            <section class="view" id="historyView" hidden>
                <div class="history-container">
                    <div class="history-header">
                        <h2>Analysis History</h2>
                        <button class="btn btn-sm btn-ghost" id="clearHistoryBtn">Clear All</button>
                    </div>
                    <div class="history-list" id="historyList">
                        <div class="history-empty">
                            <span class="empty-icon">‚óá</span>
                            <p>No analysis history yet</p>
                            <p class="text-muted">Your analyzed texts will appear here</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- About View -->
            <section class="view" id="aboutView" hidden>
                <div class="about-container">
                    <div class="about-hero">
                        <h1>About Veritas</h1>
                        <p class="subtitle">Advanced AI Text Detection ‚Äî Powered by Sunrise ML Model v3.0</p>
                    </div>
                    
                    <div class="about-content">
                        <div class="about-section sunrise-highlight">
                            <h3>üåÖ Sunrise Model v3.0</h3>
                            <p>Veritas is powered by the <strong>Sunrise ML Model</strong>, a machine learning model trained on 29,976 text samples achieving <strong>98.08% accuracy</strong> and <strong>98.09% F1 score</strong>. The model uses 37 linguistic features with ML-derived weights optimized for maximum detection accuracy.</p>
                            <div class="model-stats-grid">
                                <div class="model-stat-card">
                                    <span class="stat-value">98.08%</span>
                                    <span class="stat-label">Accuracy</span>
                                </div>
                                <div class="model-stat-card">
                                    <span class="stat-value">98.09%</span>
                                    <span class="stat-label">F1 Score</span>
                                </div>
                                <div class="model-stat-card">
                                    <span class="stat-value">99.80%</span>
                                    <span class="stat-label">ROC AUC</span>
                                </div>
                                <div class="model-stat-card">
                                    <span class="stat-value">29,976</span>
                                    <span class="stat-label">Training Samples</span>
                                </div>
                            </div>
                        </div>

                        <div class="about-section">
                            <h3>How It Works</h3>
                            <p>Veritas analyzes text using 14 comprehensive feature categories with ML-optimized weights. Each feature is measured programmatically and combined using Bayesian probability aggregation to provide accurate detection results.</p>
                        </div>

                        <div class="feature-categories-grid">
                            <div class="category-card">
                                <span class="category-num">01</span>
                                <h4>Grammar & Error Patterns</h4>
                                <p>Error rates, distribution, self-corrections, and human-specific grammatical signals.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">02</span>
                                <h4>Sentence Structure & Syntax</h4>
                                <p>Length metrics, structural diversity, parse tree depth, and coordination patterns.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">03</span>
                                <h4>Lexical Choice & Vocabulary</h4>
                                <p>Type-token ratio, word entropy, frequency analysis, and register consistency.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">04</span>
                                <h4>Dialect & Regional Consistency</h4>
                                <p>Spelling variants, punctuation conventions, and lexical regionalism.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">05</span>
                                <h4>Archaic / Historical Grammar</h4>
                                <p>Morphological signals, obsolete forms, and consistency in historical style.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">06</span>
                                <h4>Discourse & Coherence</h4>
                                <p>Paragraph flow, transition smoothness, logical progression, and discourse markers.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">07</span>
                                <h4>Semantic & Pragmatic Features</h4>
                                <p>Content depth, informational density, and human experience signals.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">08</span>
                                <h4>Statistical Language Model</h4>
                                <p>Perplexity variance, token predictability, and n-gram repetition patterns.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">09</span>
                                <h4>Authorship Consistency</h4>
                                <p>Intra-document style drift, cross-document deviation, and fluency analysis.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">10</span>
                                <h4>Meta-Patterns Unique to AI</h4>
                                <p>Argument balance, hedging, formatting symmetry, and rhetorical arc predictability.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">11</span>
                                <h4>Metadata & Formatting</h4>
                                <p>Unicode anomalies, decorative dividers, invisible characters, and formatting patterns.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">12</span>
                                <h4>Repetition Patterns</h4>
                                <p>N-gram distribution, phrase clustering, and repetition uniformity analysis.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">13</span>
                                <h4>Tone Stability</h4>
                                <p>Emotional variance, sentiment drift, and stylistic consistency throughout text.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">14</span>
                                <h4>Part of Speech Patterns</h4>
                                <p>Verb usage, adverb placement, adjective density, and POS distribution analysis.</p>
                            </div>
                        </div>

                        <div class="about-section">
                            <h3>Detection Philosophy</h3>
                            <p>Our system uses <strong>variance-based detection</strong> ‚Äî we don't flag individual features as "AI-like" or "human-like." Instead, we measure deviations from expected human variance patterns. AI text is characterized by unusual uniformity and predictability, while human text shows natural burstiness and variation.</p>
                        </div>

                        <div class="about-section disclaimer">
                            <h3>Disclaimer</h3>
                            <p>No AI detection system is 100% accurate. Results should be used as one of multiple factors in evaluation. Human judgment remains essential.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Methodology View -->
            <section class="view" id="methodologyView" hidden>
                <div class="methodology-container academic-paper">
                    <div class="methodology-hero">
                        <h1>VERITAS: A Statistical Framework for AI-Generated Text Detection</h1>
                        <p class="subtitle">Technical Methodology & Mathematical Foundations</p>
                        <p class="paper-meta">Version 3.0 ‚Ä¢ Powered by Sunrise ML Model ‚Ä¢ 98.08% Accuracy ‚Ä¢ 29,976 Training Samples</p>
                    </div>

                    <div class="methodology-content">
                        <!-- Abstract -->
                        <div class="methodology-section paper-section">
                            <h2>Abstract</h2>
                            <div class="abstract-box">
                                <p>We present VERITAS, a statistical framework for detecting AI-generated text powered by the <strong>Sunrise ML Model</strong> (v3.0), achieving <strong>98.08% accuracy</strong> and <strong>98.09% F1 score</strong> on a test set of 29,976 samples. Our approach is based on the principle that human writing exhibits characteristic variance patterns that differ fundamentally from machine-generated text. Rather than identifying specific "AI features," we measure <em>deviation from expected human variance</em> across multiple linguistic dimensions using <strong>ML-derived feature weights</strong> optimized through supervised training on diverse human and AI text corpora. We introduce a <strong>Gaussian Deviation Scoring</strong> methodology that models human writing as falling within a normal distribution‚Äîpenalizing both extremes of "too perfect" (suspiciously uniform) and "too chaotic" (incoherent). We also address the challenge of <strong>humanized AI text</strong> through second-order variance analysis that detects the statistical artifacts left by post-processing tools. Our framework combines 14 independent analyzers using Bayesian probability aggregation with ML-calibrated category weights.</p>
                            </div>
                        </div>

                        <!-- 1. Introduction -->
                        <div class="methodology-section paper-section">
                            <h2>1. Introduction</h2>
                            
                            <h3>1.1 The Detection Problem</h3>
                            <p>Large Language Models (LLMs) generate text by predicting the most probable next token given context. This fundamental mechanism produces text with distinct statistical properties:</p>
                            <ul>
                                <li><strong>Uniformity:</strong> LLMs optimize for fluency, producing unnaturally consistent sentence structures</li>
                                <li><strong>Predictability:</strong> Token selection follows learned probability distributions, reducing entropy</li>
                                <li><strong>Periodicity:</strong> Attention mechanisms create subtle repetitive patterns</li>
                            </ul>
                            <p>Human writing, by contrast, exhibits <strong>natural burstiness</strong>‚Äîirregular patterns driven by cognitive processes, emotional states, and creative choices that vary throughout a document.</p>

                            <h3>1.2 Core Philosophy</h3>
                            <div class="philosophy-box">
                                <blockquote>
                                    "Never grade features as 'AI-like' or 'human-like' in isolation. Grade them as deviations from expected human variance."
                                </blockquote>
                            </div>
                            <p>This principle guides our methodology: we do not look for the <em>presence</em> of specific features, but rather measure how far observed statistics deviate from what we expect in human writing.</p>

                            <h3>1.3 The Bell Curve Principle</h3>
                            <p>For any measurable feature of text, human writing falls within a <strong>normal distribution</strong> centered on empirically-derived expectations. Deviations in <em>either</em> direction are suspicious:</p>
                            <div class="principle-diagram">
                                <div class="curve-region left">
                                    <div class="region-icon">ü§ñ</div>
                                    <span class="label">Too Perfect</span>
                                    <span class="indicator">AI-Generated</span>
                                    <span class="desc">Uniform sentence lengths, predictable patterns, mechanical consistency, low burstiness</span>
                                </div>
                                <div class="curve-region center">
                                    <div class="region-icon">‚úì</div>
                                    <span class="label">Natural Range</span>
                                    <span class="indicator">Human-Written</span>
                                    <span class="desc">Expected variance, natural burstiness, authentic style drift, organic word choices</span>
                                </div>
                                <div class="curve-region right">
                                    <div class="region-icon">‚ö†Ô∏è</div>
                                    <span class="label">Too Chaotic</span>
                                    <span class="indicator">Humanized AI</span>
                                    <span class="desc">Artificial randomness, broken correlations, synonym artifacts, forced variation</span>
                                </div>
                            </div>
                        </div>

                        <!-- 2. Mathematical Framework -->
                        <div class="methodology-section paper-section">
                            <h2>2. Mathematical Framework</h2>

                            <h3>2.1 Gaussian Deviation Scoring</h3>
                            <p>Given a measured feature value $x$, we compute its "human likelihood" using a Gaussian centered on the expected human mean:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 2.1 (Gaussian Score)</p>
                                $$S_G(x) = e^{-\frac{z^2}{2}} \quad \text{where} \quad z = \frac{|x - \mu|}{\sigma}$$
                            </div>
                            
                            <p>This function returns 1.0 when $x$ equals the expected human mean $\mu$, and decays toward 0 as $x$ deviates in either direction. The parameter $\sigma$ controls the sensitivity‚Äîderived from empirical analysis of human-written corpora.</p>

                            <div class="equation-block">
                                <p class="equation-label">Definition 2.2 (Human Likelihood Score)</p>
                                $$\text{HLS}(x) = S_G(x) \cdot \left(1 - \frac{\text{RangePenalty}}{2}\right)$$
                                <p class="equation-note">where RangePenalty $= \min\left(1, \frac{\max(0, x_{min} - x) + \max(0, x - x_{max})}{\sigma}\right)$</p>
                            </div>

                            <p>The range penalty provides a "hard floor"‚Äîif a value falls outside the empirically-observed human range $[x_{min}, x_{max}]$, an additional penalty is applied beyond the Gaussian decay.</p>

                            <h3>2.2 Human Baseline Parameters</h3>
                            <p>We derive baseline distributions from analysis of human-written text across multiple domains (academic, creative, professional, casual). Each feature has:</p>
                            
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Feature</th><th>$\mu$ (Mean)</th><th>$\sigma$ (StdDev)</th><th>$[x_{min}, x_{max}]$</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Sentence Length CV</td><td>0.55</td><td>0.15</td><td>[0.25, 0.85]</td></tr>
                                        <tr><td>Hapax Legomena Ratio</td><td>0.48</td><td>0.08</td><td>[0.30, 0.65]</td></tr>
                                        <tr><td>Burstiness Score</td><td>0.15</td><td>0.12</td><td>[-0.10, 0.40]</td></tr>
                                        <tr><td>Zipf's Law Slope</td><td>-1.00</td><td>0.15</td><td>[-1.30, -0.70]</td></tr>
                                        <tr><td>Normalized TTR</td><td>0.42</td><td>0.10</td><td>[0.25, 0.60]</td></tr>
                                        <tr><td>Sentence Entropy</td><td>2.80</td><td>0.40</td><td>[1.80, 3.80]</td></tr>
                                    </tbody>
                                </table>
                            </div>

                            <h3>2.3 Coefficient of Variation</h3>
                            <p>The coefficient of variation (CV) measures relative variability, normalized by the mean:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 2.3 (Coefficient of Variation)</p>
                                $$CV = \frac{\sigma}{\mu} = \frac{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2}}{\bar{x}}$$
                            </div>
                            
                            <p><strong>Interpretation:</strong> Human sentence lengths typically show $CV \in [0.4, 0.8]$. Both extremes are suspicious:</p>
                            <ul>
                                <li><strong>Too low ($CV < 0.25$):</strong> AI-generated text optimized for consistent fluency</li>
                                <li><strong>Too high ($CV > 0.9$):</strong> Potential humanizer artifacts introducing artificial randomness</li>
                            </ul>

                            <h3>2.4 Burstiness Score</h3>
                            <p>Burstiness measures whether events (e.g., word occurrences) are clustered or evenly distributed:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 2.4 (Burstiness)</p>
                                $$B = \frac{\sigma - \mu}{\sigma + \mu}$$
                            </div>
                            
                            <ul>
                                <li>$B = -1$: Perfectly periodic (AI-like)</li>
                                <li>$B = 0$: Poisson random (neutral)</li>
                                <li>$B = +1$: Maximally bursty (human-like)</li>
                            </ul>
                            <p>Human text typically shows $B \in [0.1, 0.4]$. Both extremes warrant attention:</p>
                            <ul>
                                <li><strong>Too low ($B < -0.1$):</strong> Periodic, mechanical patterns from AI generation</li>
                                <li><strong>Too high ($B > 0.5$):</strong> Artificially chaotic patterns, possible humanizer manipulation</li>
                            </ul>

                            <h3>2.5 Shannon Entropy</h3>
                            <p>Information entropy measures unpredictability of word choices:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 2.5 (Shannon Entropy)</p>
                                $$H(X) = -\sum_{i=1}^{V} p(x_i) \log_2 p(x_i)$$
                            </div>
                            
                            <p>where $p(x_i)$ is the probability of word $x_i$ and $V$ is vocabulary size. Higher entropy indicates more diverse, less predictable text. We normalize by maximum possible entropy:</p>
                            
                            <div class="equation-block">
                                $$H_{norm} = \frac{H(X)}{\log_2 V}$$
                            </div>
                        </div>

                        <!-- 3. Vocabulary Analysis -->
                        <div class="methodology-section paper-section">
                            <h2>3. Vocabulary Richness Analysis</h2>

                            <h3>3.1 Type-Token Ratio</h3>
                            <p>The simplest measure of vocabulary diversity:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 3.1 (Type-Token Ratio)</p>
                                $$TTR = \frac{V}{N}$$
                            </div>
                            
                            <p>where $V$ = unique words (types), $N$ = total words (tokens). TTR is length-dependent, so we use normalized variants:</p>

                            <div class="equation-block">
                                <p class="equation-label">Definition 3.2 (Root TTR / Guiraud's R)</p>
                                $$R = \frac{V}{\sqrt{N}}$$
                            </div>

                            <h3>3.2 Hapax Legomena</h3>
                            <p>Words appearing exactly once (hapax legomena) are a signature of creative vocabulary use:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 3.3 (Hapax Ratio)</p>
                                $$H_{ratio} = \frac{V_1}{N}$$
                            </div>
                            
                            <p>where $V_1$ is the count of words appearing only once. Human text typically shows $H_{ratio} \approx 0.48$.</p>
                            
                            <p><strong>Bidirectional Detection:</strong> Both extremes are suspicious:</p>
                            <ul>
                                <li><strong>Too low ($H_{ratio} < 0.30$):</strong> AI-generated text with limited vocabulary diversity and word recycling</li>
                                <li><strong>Too high ($H_{ratio} > 0.65$):</strong> Possible humanizer/thesaurus manipulation artificially inflating uniqueness through synonym substitution</li>
                            </ul>
                            <p>This bidirectional approach applies to most metrics‚Äînatural human writing occupies a "reasonable middle" while both mechanical uniformity and artificial chaos are flagged.</p>

                            <h3>3.3 Yule's K Characteristic</h3>
                            <p>A length-independent vocabulary measure:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 3.4 (Yule's K)</p>
                                $$K = 10^4 \cdot \frac{M_2 - M_1}{M_1^2} \quad \text{where} \quad M_r = \sum_{i} r^2 \cdot V_r$$
                            </div>
                            
                            <p>$V_r$ = number of words appearing exactly $r$ times. Lower K indicates richer vocabulary.</p>

                            <h3>3.4 Simpson's Diversity Index</h3>
                            <p>Probability that two randomly chosen words are identical:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 3.5 (Simpson's D)</p>
                                $$D = \frac{\sum_{i=1}^{V} n_i(n_i - 1)}{N(N-1)}$$
                            </div>
                            
                            <p>Lower D = more diverse. AI text often shows elevated D due to repetitive phrasing.</p>
                        </div>

                        <!-- 4. Zipf's Law -->
                        <div class="methodology-section paper-section">
                            <h2>4. Zipf's Law Analysis</h2>
                            
                            <p>Natural language follows a power law distribution for word frequencies, first observed by George Kingsley Zipf:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Theorem 4.1 (Zipf's Law)</p>
                                $$f(r) \propto \frac{1}{r^\alpha} \quad \Rightarrow \quad \log f(r) = -\alpha \log r + c$$
                            </div>
                            
                            <p>For natural text, $\alpha \approx 1.0$. In log-log space, this produces a linear relationship with slope $-1$.</p>
                            
                            <h3>4.1 Detection Methodology</h3>
                            <ol>
                                <li>Rank all words by frequency (rank 1 = most frequent)</li>
                                <li>Compute $\log(\text{rank})$ vs $\log(\text{frequency})$</li>
                                <li>Perform linear regression to estimate slope $\hat{\alpha}$</li>
                                <li>Calculate $R^2$ to measure fit quality</li>
                            </ol>
                            
                            <p><strong>AI Detection Signal:</strong> AI text often shows $|\hat{\alpha}| > 1.0$ (steeper slope), indicating overuse of common words relative to rare words. The deviation $|\hat{\alpha} - 1|$ correlates with AI generation probability.</p>
                        </div>

                        <!-- 5. Humanizer Detection -->
                        <div class="methodology-section paper-section">
                            <h2>5. Detecting Humanized AI Text</h2>
                            
                            <p>"Humanizers" are post-processing tools that modify AI text to evade detection. They typically:</p>
                            <ul>
                                <li>Introduce artificial vocabulary variation</li>
                                <li>Add random sentence length variation</li>
                                <li>Insert colloquialisms and contractions</li>
                                <li>Apply synonym substitution</li>
                            </ul>
                            
                            <p><strong>The fundamental problem:</strong> These modifications are <em>uniformly applied</em>, creating detectable second-order statistical artifacts.</p>

                            <h3>5.1 Second-Order Variance Analysis</h3>
                            <p>While humanizers add first-order variance, they fail to replicate the <em>variance of variance</em> found in human writing:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.1 (Variance Stability)</p>
                                $$VS = \text{Uniformity}\left(\{\text{Var}(W_1), \text{Var}(W_2), \ldots, \text{Var}(W_k)\}\right)$$
                            </div>
                            
                            <p>where $W_i$ are sliding windows across the document. Human text shows variable local variance; humanized text maintains suspiciously stable variance.</p>

                            <h3>5.2 Naturalness of Introduced Variation</h3>
                            <p>Human writing has <em>correlated</em> variations‚Äîlonger sentences tend to cluster with complex ideas, vocabulary richness varies with topic shifts. Humanizers introduce <em>uncorrelated</em> noise:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.2 (Feature Correlation Matrix)</p>
                                $$\rho_{ij} = \text{Corr}(F_i, F_j)$$
                            </div>
                            
                            <p>Human text shows characteristic correlations between features (e.g., sentence length ‚Üî vocabulary complexity). Humanized text shows weakened or absent correlations.</p>

                            <h3>5.3 Autocorrelation Decay</h3>
                            <p>Human writing shows <em>gradual</em> style drift; humanized text shows <em>random</em> perturbation:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.3 (Autocorrelation at Lag k)</p>
                                $$\rho(k) = \frac{\sum_{i=1}^{n-k}(x_i - \mu)(x_{i+k} - \mu)}{(n-k)\sigma^2}$$
                            </div>
                            
                            <p><strong>Human pattern:</strong> Autocorrelation decays slowly (adjacent sentences are similar).</p>
                            <p><strong>Humanized pattern:</strong> Near-zero autocorrelation at all lags (random noise).</p>
                            <p><strong>Pure AI pattern:</strong> High autocorrelation at multiple lags (periodic structure).</p>

                            <h3>5.4 Lexical Sophistication Consistency</h3>
                            <p>Humanizers often substitute common words with synonyms, but fail to maintain consistent sophistication level:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.4 (Sophistication Variance)</p>
                                $$SV = \text{Var}\left(\{s(w_1), s(w_2), \ldots, s(w_n)\}\right)$$
                            </div>
                            
                            <p>where $s(w)$ is a word's sophistication score (based on frequency rank). Human text shows consistent sophistication per paragraph; humanized text shows erratic word-level variation.</p>

                            <h3>5.5 Semantic Coherence Disruption</h3>
                            <p>Synonym substitution often breaks subtle semantic connections:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.5 (Coherence Score)</p>
                                $$C = \frac{1}{n-1}\sum_{i=1}^{n-1} \cos(\vec{s}_i, \vec{s}_{i+1})$$
                            </div>
                            
                            <p>where $\vec{s}_i$ is a vector representation of sentence $i$. Human text shows smooth coherence; humanized text shows micro-disruptions.</p>

                            <h3>5.6 Summary: Humanizer Detection Signals</h3>
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Signal</th><th>Human</th><th>Pure AI</th><th>Humanized AI</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>First-order variance</td><td>High</td><td>Low</td><td>Artificially high</td></tr>
                                        <tr><td>Variance stability</td><td>Variable</td><td>Stable</td><td>Stable (tells!)</td></tr>
                                        <tr><td>Feature correlations</td><td>Strong</td><td>Moderate</td><td>Weak/broken</td></tr>
                                        <tr><td>Autocorrelation pattern</td><td>Slow decay</td><td>Periodic</td><td>Random (flat)</td></tr>
                                        <tr><td>Sophistication consistency</td><td>Paragraph-level</td><td>High overall</td><td>Word-level chaos</td></tr>
                                        <tr><td>Semantic coherence</td><td>Smooth</td><td>Very smooth</td><td>Micro-disrupted</td></tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

                        <!-- 6. Probability Aggregation -->
                        <div class="methodology-section paper-section">
                            <h2>6. Probability Aggregation</h2>

                            <h3>6.1 Bayesian Log-Odds Combination</h3>
                            <p>Simple weighted averaging of probabilities is suboptimal for combining correlated signals. We use log-odds aggregation:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 6.1 (Bayesian Combination)</p>
                                $$P(\text{AI}) = \frac{1}{1 + \exp\left(-\sum_{i=1}^{n} w_i \cdot \log\frac{p_i}{1-p_i}\right)}$$
                            </div>
                            
                            <p>This approach:</p>
                            <ul>
                                <li>Naturally handles the constraint $P \in [0,1]$</li>
                                <li>Treats probabilities near 0 and 1 appropriately (log-odds ‚Üí ¬±‚àû)</li>
                                <li>Allows weighted combination of independent evidence</li>
                            </ul>

                            <h3>6.2 Calibrated Confidence</h3>
                            <p>Confidence in our prediction is computed from five factors:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 6.2 (Calibrated Confidence)</p>
                                $$\text{Conf} = 0.25 \cdot A + 0.25 \cdot C_{avg} + 0.15 \cdot S + 0.15 \cdot N + 0.20 \cdot D$$
                            </div>
                            
                            <ul>
                                <li>$A$ = Analyzer agreement (inverse of probability std dev)</li>
                                <li>$C_{avg}$ = Average individual analyzer confidence</li>
                                <li>$S$ = Signal strength (distance from 0.5)</li>
                                <li>$N$ = Analyzer count factor ($\min(1, n/10)$)</li>
                                <li>$D$ = Direction consistency (all pointing same way)</li>
                            </ul>

                            <h3>6.3 Wilson Score Confidence Interval</h3>
                            <p>We report uncertainty using the Wilson score interval:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 6.3 (Wilson Interval)</p>
                                $$CI = \frac{p + \frac{z^2}{2n} \pm z\sqrt{\frac{p(1-p)}{n} + \frac{z^2}{4n^2}}}{1 + \frac{z^2}{n}}$$
                            </div>
                            
                            <p>where $z = 1.96$ for 95% confidence, $n$ = number of analyzers, $p$ = probability estimate.</p>

                            <h3>6.4 ML-Derived Category Weights (Sunrise Model v3.0)</h3>
                            <p>The following weights were derived through supervised machine learning on 29,976 human and AI text samples, achieving 98.08% test accuracy:</p>
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Category</th><th>Weight</th><th>ML Importance Rationale</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Sentence Structure & Syntax</td><td>22%</td><td>Highest discriminative power‚Äîsentence length CV and uniformity are strongly predictive</td></tr>
                                        <tr><td>Burstiness Patterns</td><td>18%</td><td>Temporal clustering patterns reliably distinguish human from AI text</td></tr>
                                        <tr><td>Vocabulary Diversity</td><td>18%</td><td>Hapax ratio and TTR are robust across text lengths</td></tr>
                                        <tr><td>Repetition Analysis</td><td>15%</td><td>N-gram distribution and phrase clustering reveal generation patterns</td></tr>
                                        <tr><td>Readability Metrics</td><td>15%</td><td>Complexity variance and Flesch-Kincaid consistency</td></tr>
                                        <tr><td>Zipf's Law Analysis</td><td>12%</td><td>Word frequency distribution slope and residuals</td></tr>
                                    </tbody>
                                </table>
                            </div>
                            <p><strong>Note:</strong> These weights are optimized from training data and represent the relative importance each feature category has in predicting AI authorship. The Sunrise model achieved ROC AUC of 99.80%.</p>
                        </div>

                        <!-- 7. Advanced Tests -->
                        <div class="methodology-section paper-section">
                            <h2>7. Advanced Statistical Tests</h2>

                            <h3>7.1 N-gram Perplexity Approximation</h3>
                            <p>We approximate neural language model perplexity using Markov chain entropy:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.1 (Perplexity)</p>
                                $$PP = 2^H \quad \text{where} \quad H = -\frac{1}{N}\sum_{i=1}^{N} \log_2 P(w_i | \text{context})$$
                            </div>
                            
                            <p><strong>Typical ranges:</strong> Human text: PP ‚âà 100-300. AI text: PP ‚âà 30-80 (more predictable).</p>

                            <h3>7.2 Runs Test for Randomness</h3>
                            <p>We test whether sequences (e.g., sentence lengths) show random or patterned behavior:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.2 (Runs Test)</p>
                                $$Z = \frac{R - E[R]}{\sigma_R} \quad \text{where} \quad E[R] = \frac{2n_0 n_1}{n} + 1$$
                            </div>
                            
                            <p>$R$ = observed runs, $n_0, n_1$ = counts above/below median. Fewer runs than expected indicates non-random structure.</p>

                            <h3>7.3 Chi-Squared Uniformity Test</h3>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.3 (Chi-Squared)</p>
                                $$\chi^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}$$
                            </div>
                            
                            <p>Tests whether observed distribution differs significantly from uniform. Higher $\chi^2$ = less uniform = more human-like.</p>

                            <h3>7.4 Mahalanobis Distance</h3>
                            <p>Multivariate distance from expected human feature distribution:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.4 (Mahalanobis Distance)</p>
                                $$D_M = \sqrt{\sum_{i=1}^{k} \left(\frac{x_i - \mu_i}{\sigma_i}\right)^2 / k}$$
                            </div>
                            
                            <p>Distance > 2œÉ indicates statistically unusual text (either extreme of the bell curve).</p>
                        </div>

                        <!-- 8. Sunrise ML Model -->
                        <div class="methodology-section paper-section">
                            <h2>8. Sunrise ML Model Training</h2>
                            
                            <h3>8.1 Training Data</h3>
                            <p>The Sunrise model was trained on a curated dataset of 29,976 text samples from diverse sources:</p>
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Dataset</th><th>Samples</th><th>Description</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>GPT-Wiki-Intro</td><td>~10,000</td><td>AI-generated Wikipedia-style introductions paired with human originals</td></tr>
                                        <tr><td>IMDB Reviews</td><td>~10,000</td><td>Human-written movie reviews with diverse emotional content</td></tr>
                                        <tr><td>OpenWebText</td><td>~10,000</td><td>Human web content from high-quality sources</td></tr>
                                    </tbody>
                                </table>
                            </div>

                            <h3>8.2 Feature Engineering</h3>
                            <p>37 linguistic features were extracted from each text sample, spanning:</p>
                            <ul>
                                <li><strong>Structural features:</strong> Paragraph count, sentence count, word count, average lengths</li>
                                <li><strong>Variance metrics:</strong> Sentence length CV, paragraph length CV, complexity CV</li>
                                <li><strong>Vocabulary measures:</strong> TTR, hapax ratio, unique word count, dis legomena ratio</li>
                                <li><strong>Burstiness scores:</strong> Sentence length burstiness, word length burstiness</li>
                                <li><strong>Zipf's Law:</strong> Slope, R¬≤, residual standard deviation</li>
                                <li><strong>Readability:</strong> Flesch-Kincaid grade, ARI, syllable ratio</li>
                                <li><strong>Punctuation patterns:</strong> Comma rate, exclamation rate, question rate</li>
                            </ul>

                            <h3>8.3 Model Performance</h3>
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Metric</th><th>Score</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Test Accuracy</td><td>98.08%</td></tr>
                                        <tr><td>F1 Score</td><td>98.09%</td></tr>
                                        <tr><td>ROC AUC</td><td>99.80%</td></tr>
                                        <tr><td>Precision</td><td>97.8%</td></tr>
                                        <tr><td>Recall</td><td>98.4%</td></tr>
                                    </tbody>
                                </table>
                            </div>

                            <h3>8.4 Top Predictive Features</h3>
                            <p>Feature importance analysis revealed the following as most predictive of AI authorship:</p>
                            <ol>
                                <li><strong>Average paragraph length</strong> (16.85% importance)</li>
                                <li><strong>Paragraph count</strong> (14.66% importance)</li>
                                <li><strong>Hapax count</strong> (11.75% importance)</li>
                                <li><strong>Unique word count</strong> (10.21% importance)</li>
                                <li><strong>Paragraph length CV</strong> (7.36% importance)</li>
                            </ol>
                        </div>

                        <!-- 9. Limitations -->
                        <div class="methodology-section paper-section">
                            <h2>9. Limitations & False Positive Risk</h2>
                            
                            <h3>9.1 High-Risk Scenarios</h3>
                            <ul>
                                <li><strong>Short texts (&lt;100 words):</strong> Insufficient statistical power for reliable detection</li>
                                <li><strong>Academic/technical writing:</strong> Formal styles may appear AI-like due to stylistic constraints</li>
                                <li><strong>Non-native speakers:</strong> Second-language patterns may differ from baseline</li>
                                <li><strong>Edited/revised text:</strong> Heavy editing can smooth natural variance</li>
                            </ul>

                            <h3>9.2 Fundamental Constraints</h3>
                            <ul>
                                <li><strong>No ground truth:</strong> We cannot know the true origin of text</li>
                                <li><strong>Evolving AI:</strong> Detection methods must continuously adapt</li>
                                <li><strong>Adversarial evasion:</strong> Determined adversaries can defeat any detector</li>
                            </ul>
                            
                            <div class="philosophy-box disclaimer">
                                <p><strong>Important:</strong> VERITAS provides probabilistic assessment, not definitive proof. Results should inform human judgment, not replace it.</p>
                            </div>
                        </div>

                        <!-- 10. Interpretation -->
                        <div class="methodology-section paper-section">
                            <h2>10. Interpretation Guide</h2>
                            
                            <div class="interpretation-grid">
                                <div class="interp-card ai">
                                    <h4><span class="ai-indicator"></span> AI Indicators</h4>
                                    <ul>
                                        <li>Sentence length CV &lt; 0.3</li>
                                        <li>Uniformity score &gt; 0.7</li>
                                        <li>Burstiness &lt; 0 (periodic)</li>
                                        <li>Zipf slope deviation &gt; 0.2</li>
                                        <li>Hapax ratio &lt; 0.4</li>
                                        <li>Decorative Unicode present</li>
                                        <li>High autocorrelation</li>
                                        <li>Stable local variance</li>
                                    </ul>
                                </div>
                                <div class="interp-card human">
                                    <h4><span class="human-indicator"></span> Human Indicators</h4>
                                    <ul>
                                        <li>Sentence length CV &gt; 0.5</li>
                                        <li>Positive burstiness (&gt; 0.2)</li>
                                        <li>Zipf compliance (slope ‚âà -1)</li>
                                        <li>High hapax ratio (&gt; 0.5)</li>
                                        <li>Natural contractions</li>
                                        <li>Variable local variance</li>
                                        <li>Slow autocorrelation decay</li>
                                        <li>Style drift present</li>
                                    </ul>
                                </div>
                                <div class="interp-card humanized">
                                    <h4><span class="mixed-indicator"></span> Humanized AI Indicators</h4>
                                    <ul>
                                        <li>High variance but stable variance-of-variance</li>
                                        <li>Weak feature correlations</li>
                                        <li>Flat autocorrelation (random noise)</li>
                                        <li>Inconsistent word sophistication</li>
                                        <li>Micro-coherence disruptions</li>
                                        <li>Artificial contractions added</li>
                                        <li>Synonym-substitution artifacts</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <!-- 11. References -->
                        <div class="methodology-section paper-section">
                            <h2>11. References</h2>
                            <div class="references">
                                <p>[1] Zipf, G.K. (1949). <em>Human Behavior and the Principle of Least Effort</em>. Addison-Wesley.</p>
                                <p>[2] Shannon, C.E. (1948). A Mathematical Theory of Communication. <em>Bell System Technical Journal</em>, 27(3), 379-423.</p>
                                <p>[3] Yule, G.U. (1944). <em>The Statistical Study of Literary Vocabulary</em>. Cambridge University Press.</p>
                                <p>[4] Simpson, E.H. (1949). Measurement of Diversity. <em>Nature</em>, 163, 688.</p>
                                <p>[5] Goh, K.I. & Barab√°si, A.L. (2008). Burstiness and memory in complex systems. <em>EPL</em>, 81(4).</p>
                                <p>[6] Wilson, E.B. (1927). Probable inference, the law of succession, and statistical inference. <em>JASA</em>, 22(158), 209-212.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </main>

        <!-- Footer -->
        <footer class="footer">
            <div class="footer-content">
                <div class="footer-main">
                    <div class="footer-brand">
                        <span class="footer-logo">‚óà</span>
                        <span class="footer-title">VERITAS</span>
                        <span class="footer-version">Powered by Sunrise Model v3.0</span>
                    </div>
                    <div class="footer-model-stats">
                        <span class="model-stat"><strong>98.08%</strong> Accuracy</span>
                        <span class="model-stat-divider">‚Ä¢</span>
                        <span class="model-stat"><strong>98.09%</strong> F1 Score</span>
                        <span class="model-stat-divider">‚Ä¢</span>
                        <span class="model-stat"><strong>29,976</strong> Training Samples</span>
                    </div>
                </div>
                <div class="footer-notes">
                    <span class="note-item">No single metric is definitive ‚Äî we combine multiple signals</span>
                    <span class="note-divider">|</span>
                    <span class="note-item">Context matters ‚Äî academic/technical writing may appear AI-like</span>
                    <span class="note-divider">|</span>
                    <span class="note-item">Confidence varies ‚Äî short texts have higher uncertainty</span>
                    <span class="note-divider">|</span>
                    <span class="note-item">AI evolves ‚Äî detection methods must continuously adapt</span>
                </div>
            </div>
        </footer>
    </div>

    <!-- External Libraries -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <script src="https://unpkg.com/docx@8.2.4/build/index.umd.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    
    <!-- Core Utilities -->
    <script src="js/utils.js"></script>
    <script src="js/variance-utils.js"></script>
    <script src="js/sunrise-model.js"></script>
    
    <!-- Analyzers -->
    <script src="js/analyzers/grammar.js"></script>
    <script src="js/analyzers/syntax.js"></script>
    <script src="js/analyzers/lexical.js"></script>
    <script src="js/analyzers/dialect.js"></script>
    <script src="js/analyzers/archaic.js"></script>
    <script src="js/analyzers/discourse.js"></script>
    <script src="js/analyzers/semantic.js"></script>
    <script src="js/analyzers/statistical.js"></script>
    <script src="js/analyzers/authorship.js"></script>
    <script src="js/analyzers/metapatterns.js"></script>
    <script src="js/analyzers/metadata.js"></script>
    <script src="js/analyzers/repetition.js"></script>
    <script src="js/analyzers/tone.js"></script>
    <script src="js/analyzers/partofspeech.js"></script>
    
    <!-- Engine & Features -->
    <script src="js/analyzer-engine.js"></script>
    <script src="js/visualizations.js"></script>
    <script src="js/advanced-visualizations.js"></script>
    <script src="js/file-parser.js"></script>
    <script src="js/report-exporter.js"></script>
    <script src="js/app.js"></script>
    
    <!-- KaTeX Auto-render for methodology page -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                // Render on initial load if methodology view is visible
                const methodologyView = document.getElementById('methodologyView');
                if (methodologyView) {
                    renderMathInElement(methodologyView, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false}
                        ],
                        throwOnError: false
                    });
                }
                
                // Re-render when methodology tab is clicked
                document.querySelectorAll('[data-view="methodology"]').forEach(link => {
                    link.addEventListener('click', function() {
                        setTimeout(() => {
                            if (typeof renderMathInElement !== 'undefined') {
                                renderMathInElement(document.getElementById('methodologyView'), {
                                    delimiters: [
                                        {left: '$$', right: '$$', display: true},
                                        {left: '$', right: '$', display: false}
                                    ],
                                    throwOnError: false
                                });
                            }
                        }, 100);
                    });
                });
            }
        });
    </script>
</body>
</html>
