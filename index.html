<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Veritas — AI Text Detection</title>
    <link rel="icon" type="image/svg+xml" href="favicon.svg">
    <link rel="stylesheet" href="css/style.css">
    <link rel="stylesheet" href="css/components.css">
    <link rel="stylesheet" href="css/animations.css">
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;500;600;700&family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <!-- KaTeX for mathematical typesetting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
</head>
<body>
    <div class="app-container">
        <!-- Header -->
        <header class="header">
            <div class="logo">
                <span class="logo-icon">◈</span>
                <span class="logo-text">VERITAS</span>
                <span class="logo-tagline">AI Text Detection</span>
            </div>
            <nav class="nav">
                <a href="#" class="nav-link active" data-view="analyze">Analyze</a>
                <a href="#" class="nav-link" data-view="humanizer">Humanizer</a>
                <a href="#" class="nav-link" data-view="history">History</a>
                <a href="#" class="nav-link" data-view="methodology">Methodology</a>
                <a href="#" class="nav-link" data-view="about">About</a>
            </nav>
            <div class="header-actions">
                <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
                    <span class="theme-label light-label">Light</span>
                    <span class="theme-label dark-label">Dark</span>
                </button>
            </div>
        </header>

        <!-- Main Content -->
        <main class="main-content">
            <!-- Analyze View -->
            <section class="view active" id="analyzeView">
                <div class="analyze-container">
                    <!-- Input Panel -->
                    <div class="panel input-panel">
                        <!-- Model Carousel Selector - Above Input -->
                        <div class="model-carousel-container">
                            <h3 class="model-carousel-title">
                                <span class="material-icons">wb_sunny</span>
                                Select Detection Model
                            </h3>
                            <div class="model-carousel">
                                <button class="carousel-nav carousel-prev" id="modelPrev" aria-label="Previous model">
                                    <span class="material-icons">chevron_left</span>
                                </button>
                                <div class="model-carousel-track" id="modelCarouselTrack">
                                    <div class="model-card active" data-model="helios">
                                        <input type="radio" name="model" value="helios" id="model-helios" checked hidden>
                                        <div class="model-card-icon">
                                            <span class="material-icons">flare</span>
                                        </div>
                                        <div class="model-card-content">
                                            <h4 class="model-card-name">Helios</h4>
                                            <span class="model-card-badge flagship">Flagship</span>
                                            <div class="model-card-accuracy">99.24% Accuracy</div>
                                            <ul class="model-card-features">
                                                <li><span class="material-icons">check</span> 45 linguistic features</li>
                                                <li><span class="material-icons">check</span> Tone + hedging analysis</li>
                                                <li><span class="material-icons">check</span> ROC-AUC: 99.98%</li>
                                                <li><span class="material-icons">check</span> Best overall performance</li>
                                            </ul>
                                        </div>
                                    </div>
                                    <div class="model-card" data-model="zenith">
                                        <input type="radio" name="model" value="zenith" id="model-zenith" hidden>
                                        <div class="model-card-icon">
                                            <span class="material-icons">brightness_high</span>
                                        </div>
                                        <div class="model-card-content">
                                            <h4 class="model-card-name">Zenith</h4>
                                            <span class="model-card-badge perplexity">Perplexity-Based</span>
                                            <div class="model-card-accuracy">99.57% Accuracy</div>
                                            <ul class="model-card-features">
                                                <li><span class="material-icons">check</span> Entropy analysis</li>
                                                <li><span class="material-icons">check</span> Burstiness detection</li>
                                                <li><span class="material-icons">check</span> 86.7% humanized detection</li>
                                                <li><span class="material-icons">check</span> Best for bypass tools</li>
                                            </ul>
                                        </div>
                                    </div>
                                    <div class="model-card" data-model="sunrise">
                                        <input type="radio" name="model" value="sunrise" id="model-sunrise" hidden>
                                        <div class="model-card-icon">
                                            <span class="material-icons">wb_sunny</span>
                                        </div>
                                        <div class="model-card-content">
                                            <h4 class="model-card-name">Sunrise</h4>
                                            <span class="model-card-badge balanced">Balanced</span>
                                            <div class="model-card-accuracy">98.08% Accuracy</div>
                                            <ul class="model-card-features">
                                                <li><span class="material-icons">check</span> Statistical variance</li>
                                                <li><span class="material-icons">check</span> F1: 98.09%</li>
                                                <li><span class="material-icons">check</span> Fast processing</li>
                                                <li><span class="material-icons">check</span> Well-rounded</li>
                                            </ul>
                                        </div>
                                    </div>
                                    <div class="model-card" data-model="dawn">
                                        <input type="radio" name="model" value="dawn" id="model-dawn" hidden>
                                        <div class="model-card-icon">
                                            <span class="material-icons">wb_twilight</span>
                                        </div>
                                        <div class="model-card-content">
                                            <h4 class="model-card-name">Dawn</h4>
                                            <span class="model-card-badge legacy">Legacy</span>
                                            <div class="model-card-accuracy">84.9% Accuracy</div>
                                            <ul class="model-card-features">
                                                <li><span class="material-icons">check</span> Rule-based heuristics</li>
                                                <li><span class="material-icons">check</span> Low resource usage</li>
                                                <li><span class="material-icons">check</span> Lightweight</li>
                                                <li><span class="material-icons">check</span> Baseline detector</li>
                                            </ul>
                                        </div>
                                    </div>
                                </div>
                                <button class="carousel-nav carousel-next" id="modelNext" aria-label="Next model">
                                    <span class="material-icons">chevron_right</span>
                                </button>
                            </div>
                            <div class="model-carousel-dots" id="modelCarouselDots">
                                <span class="dot active" data-model="helios"></span>
                                <span class="dot" data-model="zenith"></span>
                                <span class="dot" data-model="sunrise"></span>
                                <span class="dot" data-model="dawn"></span>
                            </div>
                        </div>

                        <div class="panel-header">
                            <h2>Input Text</h2>
                            <div class="panel-actions">
                                <button class="btn btn-sm btn-ghost" id="sampleBtn">Sample</button>
                                <button class="btn btn-sm btn-ghost" id="clearBtn">Clear</button>
                                <div class="upload-dropdown">
                                    <button class="btn btn-sm btn-ghost" id="uploadDropdownBtn">
                                        Upload ▾
                                    </button>
                                    <div class="upload-menu" id="uploadMenu">
                                        <label class="upload-option" data-type="txt">
                                            <span class="material-icons upload-icon">description</span> Text File (.txt, .md)
                                            <input type="file" accept=".txt,.md" hidden>
                                        </label>
                                        <label class="upload-option" data-type="docx">
                                            <span class="material-icons upload-icon">article</span> Word Document (.docx)
                                            <input type="file" accept=".docx" hidden>
                                        </label>
                                        <label class="upload-option" data-type="pdf">
                                            <span class="material-icons upload-icon">picture_as_pdf</span> PDF Document (.pdf)
                                            <input type="file" accept=".pdf" hidden>
                                        </label>
                                        <label class="upload-option" data-type="subtitle">
                                            <span class="material-icons upload-icon">subtitles</span> Subtitles (.vtt, .srt)
                                            <input type="file" accept=".vtt,.srt" hidden>
                                        </label>
                                        <button class="upload-option" id="pasteClipboardBtn" data-type="clipboard">
                                            <span class="material-icons upload-icon">content_paste</span> Paste from Clipboard
                                        </button>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <!-- Metadata indicators -->
                        <div class="metadata-bar" id="metadataBar" hidden>
                            <span class="metadata-source" id="sourceIndicator"></span>
                            <span class="metadata-warnings" id="metadataWarnings"></span>
                        </div>
                        
                        <div class="textarea-wrapper">
                            <textarea 
                                id="textInput" 
                                placeholder="Paste or type text to analyze for AI-generated content... (minimum 10 words)"
                                spellcheck="false"
                            ></textarea>
                            <div class="textarea-stats">
                                <span id="charCount">0 characters</span>
                                <span id="wordCount">0 words</span>
                                <span id="sentenceCount">0 sentences</span>
                            </div>
                        </div>
                        <div class="analyze-actions">
                            <button class="btn btn-primary btn-lg" id="analyzeBtn" disabled>
                                <span class="btn-text">Analyze Text</span>
                                <span class="btn-spinner"></span>
                            </button>
                        </div>
                    </div>

                    <!-- Results Panel -->
                    <div class="panel results-panel">
                        <div class="results-placeholder">
                            <div class="placeholder-icon">◇</div>
                            <p>Enter text and click "Analyze" to begin detection</p>
                        </div>

                        <!-- Results Content -->
                        <div class="results-content">
                            <!-- Overall Score -->
                            <div class="score-card">
                                <div class="score-header">
                                    <h3>Detection Result</h3>
                                    <span class="model-indicator" id="resultModelIndicator"></span>
                                    <div class="text-stats"></div>
                                </div>
                                <div class="score-display">
                                    <div class="overall-score">
                                        <div class="score-ring-container"></div>
                                        <div class="verdict">
                                            <span class="verdict-label"></span>
                                            <span class="verdict-description"></span>
                                        </div>
                                        <div class="confidence">
                                            <span class="confidence-label">Confidence:</span>
                                            <span class="confidence-value">-</span>
                                        </div>
                                    </div>
                                </div>
                                <!-- Certainty Curve -->
                                <div class="certainty-curve-container" id="certaintyCurve"></div>
                                <!-- Verbose Conclusion -->
                                <div class="verbose-conclusion" id="verboseConclusion"></div>
                                <!-- Humanization Advisory -->
                                <div class="humanization-advisory" id="humanizationAdvisory"></div>
                            </div>

                            <!-- Key Findings -->
                            <div class="findings-card">
                                <h4>Key Findings</h4>
                                <div id="findingsList" class="findings-container"></div>
                            </div>

                            <!-- Results Tabs -->
                            <div class="results-tabs">
                                <div class="tabs" role="tablist">
                                    <button class="tab-btn active" data-tab="tab-highlighted" role="tab" aria-selected="true">Highlighted Text</button>
                                    <button class="tab-btn" data-tab="tab-features" role="tab" aria-selected="false">Feature Analysis</button>
                                    <button class="tab-btn" data-tab="tab-statistics" role="tab" aria-selected="false">Statistics</button>
                                    <button class="tab-btn" data-tab="tab-graphs" role="tab" aria-selected="false">Probability Graphs</button>
                                    <button class="tab-btn" data-tab="tab-report" role="tab" aria-selected="false">Detailed Report</button>
                                </div>

                                <!-- Tab Panels -->
                                <div class="tab-panel active" id="tab-highlighted" role="tabpanel">
                                    <div id="highlightedText" class="highlighted-text-wrapper"></div>
                                </div>

                                <div class="tab-panel" id="tab-features" role="tabpanel" hidden>
                                    <div id="featureAnalysis" class="features-container"></div>
                                </div>

                                <div class="tab-panel" id="tab-statistics" role="tabpanel" hidden>
                                    <div id="advancedStatistics" class="statistics-container"></div>
                                </div>

                                <div class="tab-panel" id="tab-graphs" role="tabpanel" hidden>
                                    <div id="probabilityGraphs" class="graphs-container"></div>
                                    <div id="advancedGraphs" class="graphs-container advanced-graphs">
                                        <div class="graph-section">
                                            <h4>Sentence Length Distribution</h4>
                                            <div id="sentenceLengthHistogram" class="graph-wrapper"></div>
                                        </div>
                                        <div class="graph-section">
                                            <h4>N-gram Repetition Heatmap</h4>
                                            <div id="ngramHeatmap" class="graph-wrapper"></div>
                                        </div>
                                        <div class="graph-section">
                                            <h4>Tone Stability Timeline</h4>
                                            <div id="toneTimeline" class="graph-wrapper"></div>
                                        </div>
                                        <div class="graph-section">
                                            <h4>Word Frequency Distribution</h4>
                                            <div id="zipfChart" class="graph-wrapper"></div>
                                        </div>
                                        <div class="graph-section">
                                            <h4>Feature Contributions</h4>
                                            <div id="featureContributionChart" class="graph-wrapper"></div>
                                        </div>
                                    </div>
                                </div>

                                <div class="tab-panel" id="tab-report" role="tabpanel" hidden>
                                    <div id="detailedReport" class="report-container"></div>
                                </div>
                            </div>

                            <!-- Export Actions -->
                            <div class="export-actions">
                                <button class="btn btn-sm btn-ghost" id="copyResultsBtn">Copy Results</button>
                                <button class="btn btn-sm btn-primary" id="viewFullReportBtn">
                                    View Full Report
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Humanizer View -->
            <section class="view" id="humanizerView" hidden>
                <div class="humanizer-container">
                    <div class="humanizer-header">
                        <h2><span class="material-icons">auto_fix_high</span> Text Humanizer</h2>
                        <p class="subtitle">Transform AI-generated text to appear more human-like</p>
                        <p class="disclaimer"><span class="material-icons" style="font-size:14px;vertical-align:middle">warning</span> For educational and testing purposes only. Use responsibly.</p>
                    </div>
                    
                    <div class="humanizer-content">
                        <div class="humanizer-input-panel">
                            <h3>Input Text</h3>
                            <textarea id="humanizerInput" placeholder="Paste AI-generated text here..."></textarea>
                            <div class="humanizer-input-stats" id="humanizerInputStats">0 characters | 0 words</div>
                            
                            <div class="humanizer-options">
                                <label class="humanizer-option">
                                    <span>Intensity:</span>
                                    <select id="humanizeIntensity">
                                        <option value="light">Light - Subtle changes</option>
                                        <option value="medium" selected>Medium - Balanced</option>
                                        <option value="heavy">Heavy - Major rewrite</option>
                                        <option value="stealth">Stealth - Professional bypass</option>
                                    </select>
                                </label>
                                
                                <label class="humanizer-option">
                                    <span>Style Profile:</span>
                                    <select id="humanizeStyle">
                                        <option value="random" selected>Random Mix</option>
                                        <option value="casual">Casual/Conversational</option>
                                        <option value="academic">Academic/Formal</option>
                                        <option value="professional">Professional</option>
                                    </select>
                                </label>
                            </div>
                            
                            <button class="btn btn-primary" id="humanizeBtn">
                                <span class="btn-text">Humanize Text</span>
                                <span class="btn-spinner"></span>
                            </button>
                        </div>
                        
                        <div class="humanizer-output-panel">
                            <h3>Humanized Output</h3>
                            <textarea id="humanizerOutput" readonly placeholder="Humanized text will appear here..."></textarea>
                            
                            <div class="humanize-stats" id="humanizeStats"></div>
                            
                            <div class="humanized-actions" id="humanizedActions">
                                <button class="btn btn-ghost" id="copyHumanizedBtn">Copy to Clipboard</button>
                                <button class="btn btn-ghost" id="analyzeHumanizedBtn">Analyze Result</button>
                            </div>
                        </div>
                    </div>
                    
                    <div class="humanizer-info">
                        <h3>How It Works</h3>
                        <div class="info-grid">
                            <div class="info-card">
                                <h4>Contraction Injection</h4>
                                <p>Converts formal phrases like "do not" to "don't" and "it is" to "it's"</p>
                            </div>
                            <div class="info-card">
                                <h4>Disfluency Addition</h4>
                                <p>Adds natural fillers like "well", "basically", "I mean"</p>
                            </div>
                            <div class="info-card">
                                <h4>Synonym Variation</h4>
                                <p>Replaces words with contextually appropriate alternatives</p>
                            </div>
                            <div class="info-card">
                                <h4>Sentence Restructuring</h4>
                                <p>Varies sentence length and structure patterns</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- History View -->
            <section class="view" id="historyView" hidden>
                <div class="history-container">
                    <div class="history-header">
                        <h2>Analysis History</h2>
                        <button class="btn btn-sm btn-ghost" id="clearHistoryBtn">Clear All</button>
                    </div>
                    <div class="history-list" id="historyList">
                        <div class="history-empty">
                            <span class="empty-icon">◇</span>
                            <p>No analysis history yet</p>
                            <p class="text-muted">Your analyzed texts will appear here</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- About View -->
            <section class="view" id="aboutView" hidden>
                <div class="about-container">
                    <div class="about-hero">
                        <h1>About Veritas</h1>
                        <p class="subtitle">Advanced AI Text Detection — <span class="material-icons" style="vertical-align: middle">wb_sunny</span> Powered by the Sun Suite</p>
                    </div>
                    
                    <div class="about-content">
                        <!-- Model Suite Carousel -->
                        <div class="about-section models-showcase">
                            <h3><span class="material-icons">wb_sunny</span> The Sun Suite — Detection Models</h3>
                            <p class="section-intro">Veritas offers multiple specialized detection models, each optimized for different use cases. Select the model that best fits your needs when analyzing text.</p>
                            
                            <div class="about-model-carousel">
                                <div class="about-model-card featured">
                                    <div class="model-header helios">
                                        <span class="material-icons model-sun-icon">flare</span>
                                        <h4>Helios</h4>
                                        <span class="model-badge">Flagship</span>
                                    </div>
                                    <div class="model-accuracy">99.24%</div>
                                    <p class="model-desc-text">45-feature comprehensive analysis with tone, hedging, and rhetorical pattern detection. Our most accurate general-purpose model.</p>
                                    <ul class="model-highlights">
                                        <li>ROC-AUC: 99.98%</li>
                                        <li>45 linguistic features</li>
                                        <li>Tone & hedging analysis</li>
                                        <li>Best overall accuracy</li>
                                    </ul>
                                </div>
                                
                                <div class="about-model-card">
                                    <div class="model-header zenith">
                                        <span class="material-icons model-sun-icon">brightness_high</span>
                                        <h4>Zenith</h4>
                                        <span class="model-badge">Perplexity</span>
                                    </div>
                                    <div class="model-accuracy">99.57%</div>
                                    <p class="model-desc-text">Specialized entropy and burstiness analysis optimized for detecting AI text modified by humanizer tools.</p>
                                    <ul class="model-highlights">
                                        <li>86.7% humanized detection</li>
                                        <li>Entropy-focused analysis</li>
                                        <li>Burstiness metrics</li>
                                        <li>Best for bypass tools</li>
                                    </ul>
                                </div>
                                
                                <div class="about-model-card">
                                    <div class="model-header sunrise">
                                        <span class="material-icons model-sun-icon">wb_sunny</span>
                                        <h4>Sunrise</h4>
                                        <span class="model-badge">Balanced</span>
                                    </div>
                                    <div class="model-accuracy">98.08%</div>
                                    <p class="model-desc-text">Statistical variance-based detection with balanced precision and recall. Fast and reliable for general use.</p>
                                    <ul class="model-highlights">
                                        <li>F1 Score: 98.09%</li>
                                        <li>Fast processing</li>
                                        <li>Well-rounded metrics</li>
                                        <li>Good all-rounder</li>
                                    </ul>
                                </div>
                                
                                <div class="about-model-card">
                                    <div class="model-header dawn">
                                        <span class="material-icons model-sun-icon">wb_twilight</span>
                                        <h4>Dawn</h4>
                                        <span class="model-badge">Legacy</span>
                                    </div>
                                    <div class="model-accuracy">84.9%</div>
                                    <p class="model-desc-text">Rule-based heuristic detection for low-resource environments. Lightweight baseline detector.</p>
                                    <ul class="model-highlights">
                                        <li>Minimal resources</li>
                                        <li>Rule-based logic</li>
                                        <li>Fast execution</li>
                                        <li>Baseline reference</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <div class="about-section">
                            <h3>How It Works</h3>
                            <p>Veritas analyzes text using 14 comprehensive feature categories with ML-optimized weights. Each model extracts distinct features covering structural patterns, entropy analysis, lexical diversity, tone consistency, hedging behavior, personal voice markers, and rhetorical patterns. Features are combined using Bayesian probability aggregation with calibrated confidence scoring.</p>
                        </div>

                        <div class="feature-categories-grid">
                            <div class="category-card">
                                <span class="category-num">01</span>
                                <h4>Grammar & Error Patterns</h4>
                                <p>Error rates, distribution, self-corrections, and human-specific grammatical signals.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">02</span>
                                <h4>Sentence Structure & Syntax</h4>
                                <p>Length metrics, structural diversity, parse tree depth, and coordination patterns.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">03</span>
                                <h4>Lexical Choice & Vocabulary</h4>
                                <p>Type-token ratio, word entropy, frequency analysis, and register consistency.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">04</span>
                                <h4>Dialect & Regional Consistency</h4>
                                <p>Spelling variants, punctuation conventions, and lexical regionalism.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">05</span>
                                <h4>Archaic / Historical Grammar</h4>
                                <p>Morphological signals, obsolete forms, and consistency in historical style.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">06</span>
                                <h4>Discourse & Coherence</h4>
                                <p>Paragraph flow, transition smoothness, logical progression, and discourse markers.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">07</span>
                                <h4>Semantic & Pragmatic Features</h4>
                                <p>Content depth, informational density, and human experience signals.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">08</span>
                                <h4>Statistical Language Model</h4>
                                <p>Perplexity variance, token predictability, and n-gram repetition patterns.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">09</span>
                                <h4>Authorship Consistency</h4>
                                <p>Intra-document style drift, cross-document deviation, and fluency analysis.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">10</span>
                                <h4>Meta-Patterns Unique to AI</h4>
                                <p>Argument balance, hedging, formatting symmetry, and rhetorical arc predictability.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">11</span>
                                <h4>Metadata & Formatting</h4>
                                <p>Unicode anomalies, decorative dividers, invisible characters, and formatting patterns.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">12</span>
                                <h4>Repetition Patterns</h4>
                                <p>N-gram distribution, phrase clustering, and repetition uniformity analysis.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">13</span>
                                <h4>Tone Stability</h4>
                                <p>Emotional variance, sentiment drift, and stylistic consistency throughout text.</p>
                            </div>
                            <div class="category-card">
                                <span class="category-num">14</span>
                                <h4>Part of Speech Patterns</h4>
                                <p>Verb usage, adverb placement, adjective density, and POS distribution analysis.</p>
                            </div>
                        </div>

                        <div class="about-section">
                            <h3>Detection Philosophy</h3>
                            <p>Our system uses <strong>variance-based detection</strong> — we don't flag individual features as "AI-like" or "human-like." Instead, we measure deviations from expected human variance patterns using a <strong>bell curve principle</strong>: both extremes are suspicious. AI text is characterized by unusual uniformity and predictability (too perfect), while humanized AI text shows artificial randomness (too chaotic). Natural human writing falls within a "reasonable middle" range.</p>
                        </div>

                        <div class="about-section disclaimer">
                            <h3>Important Disclaimer</h3>
                            <p><strong>No AI detection system is 100% accurate.</strong> Veritas provides probabilistic assessment, not definitive proof. Results should be used as one of multiple factors in evaluation—never as the sole basis for academic misconduct accusations or employment decisions. False positives can occur with formal academic writing, non-native English speakers, or heavily edited content. Human judgment remains essential.</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Methodology View -->
            <section class="view" id="methodologyView" hidden>
                <div class="methodology-container academic-paper">
                    <div class="methodology-hero">
                        <h1>VERITAS: A Statistical Framework for AI-Generated Text Detection</h1>
                        <p class="subtitle">Technical Methodology & Mathematical Foundations</p>
                        <p class="paper-meta">Version 4.0 (Helios) • 45-Feature Analysis • 99.24% Accuracy • 3-Class Detection (Human / AI / Humanized)</p>
                    </div>

                    <div class="methodology-content">
                        <!-- Abstract -->
                        <div class="methodology-section paper-section">
                            <h2>Abstract</h2>
                            <div class="abstract-box">
                                <p>We present VERITAS, a statistical framework for detecting AI-generated text. The system offers multiple detection models: the flagship <strong>Helios Model</strong> (v4.0) achieving <strong>99.24% accuracy</strong> and <strong>99.98% ROC-AUC</strong>, the <strong>Sunrise Model</strong> (v3.0) at <strong>98.08% accuracy</strong>, and the <strong>Zenith Model</strong> specialized for humanized text detection. Our approach is based on the principle that human writing exhibits characteristic variance patterns that differ fundamentally from machine-generated text. Rather than identifying specific "AI features," we measure <em>deviation from expected human variance</em> across multiple linguistic dimensions using <strong>ML-derived feature weights</strong> optimized through supervised training on 29,976 diverse human and AI text samples. We introduce a <strong>Gaussian Deviation Scoring</strong> methodology that models human writing as falling within a normal distribution—penalizing both extremes of "too perfect" (suspiciously uniform) and "too chaotic" (incoherent). We also address the challenge of <strong>humanized AI text</strong> through second-order variance analysis that detects the statistical artifacts left by post-processing tools. The Helios model uses 45 features across tone, hedging, rhetorical patterns, and structural analysis, combined via Bayesian probability aggregation with ML-calibrated weights.</p>
                            </div>
                        </div>

                        <!-- 1. Introduction -->
                        <div class="methodology-section paper-section">
                            <h2>1. Introduction</h2>
                            
                            <h3>1.1 The Detection Problem</h3>
                            <p>Large Language Models (LLMs) generate text by predicting the most probable next token given context. This fundamental mechanism produces text with distinct statistical properties:</p>
                            <ul>
                                <li><strong>Uniformity:</strong> LLMs optimize for fluency, producing unnaturally consistent sentence structures</li>
                                <li><strong>Predictability:</strong> Token selection follows learned probability distributions, reducing entropy</li>
                                <li><strong>Periodicity:</strong> Attention mechanisms create subtle repetitive patterns</li>
                            </ul>
                            <p>Human writing, by contrast, exhibits <strong>natural burstiness</strong>—irregular patterns driven by cognitive processes, emotional states, and creative choices that vary throughout a document.</p>

                            <h3>1.2 Core Philosophy</h3>
                            <div class="philosophy-box">
                                <blockquote>
                                    "Never grade features as 'AI-like' or 'human-like' in isolation. Grade them as deviations from expected human variance."
                                </blockquote>
                            </div>
                            <p>This principle guides our methodology: we do not look for the <em>presence</em> of specific features, but rather measure how far observed statistics deviate from what we expect in human writing.</p>

                            <h3>1.3 The Bell Curve Principle</h3>
                            <p>For any measurable feature of text, human writing falls within a <strong>normal distribution</strong> centered on empirically-derived expectations. Deviations in <em>either</em> direction are suspicious:</p>
                            <div class="principle-diagram">
                                <div class="curve-region left">
                                    <div class="region-icon"><span class="material-icons">smart_toy</span></div>
                                    <span class="label">Too Perfect</span>
                                    <span class="indicator">AI-Generated</span>
                                    <span class="desc">Uniform sentence lengths, predictable patterns, mechanical consistency, low burstiness</span>
                                </div>
                                <div class="curve-region center">
                                    <div class="region-icon"><span class="material-icons">check_circle</span></div>
                                    <span class="label">Natural Range</span>
                                    <span class="indicator">Human-Written</span>
                                    <span class="desc">Expected variance, natural burstiness, authentic style drift, organic word choices</span>
                                </div>
                                <div class="curve-region right">
                                    <div class="region-icon"><span class="material-icons">help_outline</span></div>
                                    <span class="label">Too Chaotic</span>
                                    <span class="indicator">Humanized AI</span>
                                    <span class="desc">Artificial randomness, broken correlations, synonym artifacts, forced variation</span>
                                </div>
                            </div>
                        </div>

                        <!-- 2. Mathematical Framework -->
                        <div class="methodology-section paper-section">
                            <h2>2. Mathematical Framework</h2>

                            <h3>2.1 Gaussian Deviation Scoring</h3>
                            <p>Given a measured feature value $x$, we compute its "human likelihood" using a Gaussian centered on the expected human mean:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 2.1 (Gaussian Score)</p>
                                $$S_G(x) = e^{-\frac{z^2}{2}} \quad \text{where} \quad z = \frac{|x - \mu|}{\sigma}$$
                            </div>
                            
                            <p>This function returns 1.0 when $x$ equals the expected human mean $\mu$, and decays toward 0 as $x$ deviates in either direction. The parameter $\sigma$ controls the sensitivity—derived from empirical analysis of human-written corpora.</p>

                            <div class="equation-block">
                                <p class="equation-label">Definition 2.2 (Human Likelihood Score)</p>
                                $$\text{HLS}(x) = S_G(x) \cdot \left(1 - \frac{\text{RangePenalty}}{2}\right)$$
                                <p class="equation-note">where RangePenalty $= \min\left(1, \frac{\max(0, x_{min} - x) + \max(0, x - x_{max})}{\sigma}\right)$</p>
                            </div>

                            <p>The range penalty provides a "hard floor"—if a value falls outside the empirically-observed human range $[x_{min}, x_{max}]$, an additional penalty is applied beyond the Gaussian decay.</p>

                            <h3>2.2 Human Baseline Parameters</h3>
                            <p>We derive baseline distributions from analysis of human-written text across multiple domains (academic, creative, professional, casual). Each feature has:</p>
                            
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Feature</th><th>$\mu$ (Mean)</th><th>$\sigma$ (StdDev)</th><th>$[x_{min}, x_{max}]$</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Sentence Length CV</td><td>0.55</td><td>0.15</td><td>[0.25, 0.85]</td></tr>
                                        <tr><td>Hapax Legomena Ratio</td><td>0.48</td><td>0.08</td><td>[0.30, 0.65]</td></tr>
                                        <tr><td>Burstiness Score</td><td>0.15</td><td>0.12</td><td>[-0.10, 0.40]</td></tr>
                                        <tr><td>Zipf's Law Slope</td><td>-1.00</td><td>0.15</td><td>[-1.30, -0.70]</td></tr>
                                        <tr><td>Normalized TTR</td><td>0.42</td><td>0.10</td><td>[0.25, 0.60]</td></tr>
                                        <tr><td>Sentence Entropy</td><td>2.80</td><td>0.40</td><td>[1.80, 3.80]</td></tr>
                                    </tbody>
                                </table>
                            </div>

                            <h3>2.3 Coefficient of Variation</h3>
                            <p>The coefficient of variation (CV) measures relative variability, normalized by the mean:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 2.3 (Coefficient of Variation)</p>
                                $$CV = \frac{\sigma}{\mu} = \frac{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2}}{\bar{x}}$$
                            </div>
                            
                            <p><strong>Interpretation:</strong> Human sentence lengths typically show $CV \in [0.4, 0.8]$. Both extremes are suspicious:</p>
                            <ul>
                                <li><strong>Too low ($CV < 0.25$):</strong> AI-generated text optimized for consistent fluency</li>
                                <li><strong>Too high ($CV > 0.9$):</strong> Potential humanizer artifacts introducing artificial randomness</li>
                            </ul>

                            <h3>2.4 Burstiness Score</h3>
                            <p>Burstiness measures whether events (e.g., word occurrences) are clustered or evenly distributed:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 2.4 (Burstiness)</p>
                                $$B = \frac{\sigma - \mu}{\sigma + \mu}$$
                            </div>
                            
                            <ul>
                                <li>$B = -1$: Perfectly periodic (AI-like)</li>
                                <li>$B = 0$: Poisson random (neutral)</li>
                                <li>$B = +1$: Maximally bursty (human-like)</li>
                            </ul>
                            <p>Human text typically shows $B \in [0.1, 0.4]$. Both extremes warrant attention:</p>
                            <ul>
                                <li><strong>Too low ($B < -0.1$):</strong> Periodic, mechanical patterns from AI generation</li>
                                <li><strong>Too high ($B > 0.5$):</strong> Artificially chaotic patterns, possible humanizer manipulation</li>
                            </ul>

                            <h3>2.5 Shannon Entropy</h3>
                            <p>Information entropy measures unpredictability of word choices:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 2.5 (Shannon Entropy)</p>
                                $$H(X) = -\sum_{i=1}^{V} p(x_i) \log_2 p(x_i)$$
                            </div>
                            
                            <p>where $p(x_i)$ is the probability of word $x_i$ and $V$ is vocabulary size. Higher entropy indicates more diverse, less predictable text. We normalize by maximum possible entropy:</p>
                            
                            <div class="equation-block">
                                $$H_{norm} = \frac{H(X)}{\log_2 V}$$
                            </div>
                        </div>

                        <!-- 3. Vocabulary Analysis -->
                        <div class="methodology-section paper-section">
                            <h2>3. Vocabulary Richness Analysis</h2>

                            <h3>3.1 Type-Token Ratio</h3>
                            <p>The simplest measure of vocabulary diversity:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 3.1 (Type-Token Ratio)</p>
                                $$TTR = \frac{V}{N}$$
                            </div>
                            
                            <p>where $V$ = unique words (types), $N$ = total words (tokens). TTR is length-dependent, so we use normalized variants:</p>

                            <div class="equation-block">
                                <p class="equation-label">Definition 3.2 (Root TTR / Guiraud's R)</p>
                                $$R = \frac{V}{\sqrt{N}}$$
                            </div>

                            <h3>3.2 Hapax Legomena</h3>
                            <p>Words appearing exactly once (hapax legomena) are a signature of creative vocabulary use:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 3.3 (Hapax Ratio)</p>
                                $$H_{ratio} = \frac{V_1}{N}$$
                            </div>
                            
                            <p>where $V_1$ is the count of words appearing only once. Human text typically shows $H_{ratio} \approx 0.48$.</p>
                            
                            <p><strong>Bidirectional Detection:</strong> Both extremes are suspicious:</p>
                            <ul>
                                <li><strong>Too low ($H_{ratio} < 0.30$):</strong> AI-generated text with limited vocabulary diversity and word recycling</li>
                                <li><strong>Too high ($H_{ratio} > 0.65$):</strong> Possible humanizer/thesaurus manipulation artificially inflating uniqueness through synonym substitution</li>
                            </ul>
                            <p>This bidirectional approach applies to most metrics—natural human writing occupies a "reasonable middle" while both mechanical uniformity and artificial chaos are flagged.</p>

                            <h3>3.3 Yule's K Characteristic</h3>
                            <p>A length-independent vocabulary measure:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 3.4 (Yule's K)</p>
                                $$K = 10^4 \cdot \frac{M_2 - M_1}{M_1^2} \quad \text{where} \quad M_r = \sum_{i} r^2 \cdot V_r$$
                            </div>
                            
                            <p>$V_r$ = number of words appearing exactly $r$ times. Lower K indicates richer vocabulary.</p>

                            <h3>3.4 Simpson's Diversity Index</h3>
                            <p>Probability that two randomly chosen words are identical:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 3.5 (Simpson's D)</p>
                                $$D = \frac{\sum_{i=1}^{V} n_i(n_i - 1)}{N(N-1)}$$
                            </div>
                            
                            <p>Lower D = more diverse. AI text often shows elevated D due to repetitive phrasing.</p>
                        </div>

                        <!-- 4. Zipf's Law -->
                        <div class="methodology-section paper-section">
                            <h2>4. Zipf's Law Analysis</h2>
                            
                            <p>Natural language follows a power law distribution for word frequencies, first observed by George Kingsley Zipf:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Theorem 4.1 (Zipf's Law)</p>
                                $$f(r) \propto \frac{1}{r^\alpha} \quad \Rightarrow \quad \log f(r) = -\alpha \log r + c$$
                            </div>
                            
                            <p>For natural text, $\alpha \approx 1.0$. In log-log space, this produces a linear relationship with slope $-1$.</p>
                            
                            <h3>4.1 Detection Methodology</h3>
                            <ol>
                                <li>Rank all words by frequency (rank 1 = most frequent)</li>
                                <li>Compute $\log(\text{rank})$ vs $\log(\text{frequency})$</li>
                                <li>Perform linear regression to estimate slope $\hat{\alpha}$</li>
                                <li>Calculate $R^2$ to measure fit quality</li>
                            </ol>
                            
                            <p><strong>AI Detection Signal:</strong> AI text often shows $|\hat{\alpha}| > 1.0$ (steeper slope), indicating overuse of common words relative to rare words. The deviation $|\hat{\alpha} - 1|$ correlates with AI generation probability.</p>
                        </div>

                        <!-- 5. Humanizer Detection -->
                        <div class="methodology-section paper-section">
                            <h2>5. Detecting Humanized AI Text</h2>
                            
                            <p>"Humanizers" are post-processing tools that modify AI text to evade detection. They typically:</p>
                            <ul>
                                <li>Introduce artificial vocabulary variation</li>
                                <li>Add random sentence length variation</li>
                                <li>Insert colloquialisms and contractions</li>
                                <li>Apply synonym substitution</li>
                            </ul>
                            
                            <p><strong>The fundamental problem:</strong> These modifications are <em>uniformly applied</em>, creating detectable second-order statistical artifacts.</p>

                            <h3>5.1 Second-Order Variance Analysis</h3>
                            <p>While humanizers add first-order variance, they fail to replicate the <em>variance of variance</em> found in human writing:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.1 (Variance Stability)</p>
                                $$VS = \text{Uniformity}\left(\{\text{Var}(W_1), \text{Var}(W_2), \ldots, \text{Var}(W_k)\}\right)$$
                            </div>
                            
                            <p>where $W_i$ are sliding windows across the document. Human text shows variable local variance; humanized text maintains suspiciously stable variance.</p>

                            <h3>5.2 Naturalness of Introduced Variation</h3>
                            <p>Human writing has <em>correlated</em> variations—longer sentences tend to cluster with complex ideas, vocabulary richness varies with topic shifts. Humanizers introduce <em>uncorrelated</em> noise:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.2 (Feature Correlation Matrix)</p>
                                $$\rho_{ij} = \text{Corr}(F_i, F_j)$$
                            </div>
                            
                            <p>Human text shows characteristic correlations between features (e.g., sentence length ↔ vocabulary complexity). Humanized text shows weakened or absent correlations.</p>

                            <h3>5.3 Autocorrelation Decay</h3>
                            <p>Human writing shows <em>gradual</em> style drift; humanized text shows <em>random</em> perturbation:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.3 (Autocorrelation at Lag k)</p>
                                $$\rho(k) = \frac{\sum_{i=1}^{n-k}(x_i - \mu)(x_{i+k} - \mu)}{(n-k)\sigma^2}$$
                            </div>
                            
                            <p><strong>Human pattern:</strong> Autocorrelation decays slowly (adjacent sentences are similar).</p>
                            <p><strong>Humanized pattern:</strong> Near-zero autocorrelation at all lags (random noise).</p>
                            <p><strong>Pure AI pattern:</strong> High autocorrelation at multiple lags (periodic structure).</p>

                            <h3>5.4 Lexical Sophistication Consistency</h3>
                            <p>Humanizers often substitute common words with synonyms, but fail to maintain consistent sophistication level:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.4 (Sophistication Variance)</p>
                                $$SV = \text{Var}\left(\{s(w_1), s(w_2), \ldots, s(w_n)\}\right)$$
                            </div>
                            
                            <p>where $s(w)$ is a word's sophistication score (based on frequency rank). Human text shows consistent sophistication per paragraph; humanized text shows erratic word-level variation.</p>

                            <h3>5.5 Semantic Coherence Disruption</h3>
                            <p>Synonym substitution often breaks subtle semantic connections:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 5.5 (Coherence Score)</p>
                                $$C = \frac{1}{n-1}\sum_{i=1}^{n-1} \cos(\vec{s}_i, \vec{s}_{i+1})$$
                            </div>
                            
                            <p>where $\vec{s}_i$ is a vector representation of sentence $i$. Human text shows smooth coherence; humanized text shows micro-disruptions.</p>

                            <h3>5.6 Summary: Humanizer Detection Signals</h3>
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Signal</th><th>Human</th><th>Pure AI</th><th>Humanized AI</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>First-order variance</td><td>High</td><td>Low</td><td>Artificially high</td></tr>
                                        <tr><td>Variance stability</td><td>Variable</td><td>Stable</td><td>Stable (tells!)</td></tr>
                                        <tr><td>Feature correlations</td><td>Strong</td><td>Moderate</td><td>Weak/broken</td></tr>
                                        <tr><td>Autocorrelation pattern</td><td>Slow decay</td><td>Periodic</td><td>Random (flat)</td></tr>
                                        <tr><td>Sophistication consistency</td><td>Paragraph-level</td><td>High overall</td><td>Word-level chaos</td></tr>
                                        <tr><td>Semantic coherence</td><td>Smooth</td><td>Very smooth</td><td>Micro-disrupted</td></tr>
                                    </tbody>
                                </table>
                            </div>
                        </div>

                        <!-- 6. Probability Aggregation -->
                        <div class="methodology-section paper-section">
                            <h2>6. Probability Aggregation</h2>

                            <h3>6.1 Bayesian Log-Odds Combination</h3>
                            <p>Simple weighted averaging of probabilities is suboptimal for combining correlated signals. We use log-odds aggregation:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 6.1 (Bayesian Combination)</p>
                                $$P(\text{AI}) = \frac{1}{1 + \exp\left(-\sum_{i=1}^{n} w_i \cdot \log\frac{p_i}{1-p_i}\right)}$$
                            </div>
                            
                            <p>This approach:</p>
                            <ul>
                                <li>Naturally handles the constraint $P \in [0,1]$</li>
                                <li>Treats probabilities near 0 and 1 appropriately (log-odds → ±∞)</li>
                                <li>Allows weighted combination of independent evidence</li>
                            </ul>

                            <h3>6.2 Calibrated Confidence</h3>
                            <p>Confidence in our prediction is computed from five factors:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 6.2 (Calibrated Confidence)</p>
                                $$\text{Conf} = 0.25 \cdot A + 0.25 \cdot C_{avg} + 0.15 \cdot S + 0.15 \cdot N + 0.20 \cdot D$$
                            </div>
                            
                            <ul>
                                <li>$A$ = Analyzer agreement (inverse of probability std dev)</li>
                                <li>$C_{avg}$ = Average individual analyzer confidence</li>
                                <li>$S$ = Signal strength (distance from 0.5)</li>
                                <li>$N$ = Analyzer count factor ($\min(1, n/10)$)</li>
                                <li>$D$ = Direction consistency (all pointing same way)</li>
                            </ul>

                            <h3>6.3 Wilson Score Confidence Interval</h3>
                            <p>We report uncertainty using the Wilson score interval:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 6.3 (Wilson Interval)</p>
                                $$CI = \frac{p + \frac{z^2}{2n} \pm z\sqrt{\frac{p(1-p)}{n} + \frac{z^2}{4n^2}}}{1 + \frac{z^2}{n}}$$
                            </div>
                            
                            <p>where $z = 1.96$ for 95% confidence, $n$ = number of analyzers, $p$ = probability estimate.</p>

                            <h3>6.4 ML-Derived Feature Weights (Helios Model v4.0)</h3>
                            <p>The following weights were derived through supervised machine learning on 29,976 human and AI text samples, achieving 99.24% test accuracy and 99.98% ROC-AUC:</p>
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Feature</th><th>Weight</th><th>ML Importance Rationale</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Paragraph Uniformity</td><td>39.33%</td><td>Strongest predictor—AI paragraphs are unnaturally uniform in length</td></tr>
                                        <tr><td>Trigram Entropy</td><td>15.39%</td><td>3-word pattern predictability reveals generation bias</td></tr>
                                        <tr><td>Bigram Entropy</td><td>14.93%</td><td>2-word pattern diversity distinguishes human spontaneity</td></tr>
                                        <tr><td>Average Paragraph Length</td><td>6.67%</td><td>AI tends toward specific paragraph length ranges</td></tr>
                                        <tr><td>Average Sentence Length</td><td>5.08%</td><td>Sentence length consistency patterns</td></tr>
                                        <tr><td>Lexical Diversity</td><td>4.08%</td><td>Vocabulary richness and word variety</td></tr>
                                        <tr><td>Comma Rhythm Variance</td><td>3.49%</td><td>Punctuation patterns reveal mechanical generation</td></tr>
                                        <tr><td>Structural Variance</td><td>5.44%</td><td>Combined paragraph and sentence length variance</td></tr>
                                        <tr><td>Other Features (37 total)</td><td>5.59%</td><td>Tone, hedging, personal voice, rhetorical patterns</td></tr>
                                    </tbody>
                                </table>
                            </div>
                            <p><strong>Note:</strong> The Helios model uses 45 features across 7 categories: Structural, Entropy, Lexical, Tone, Hedging, Personal Voice, and Rhetorical patterns. Feature weights are automatically calibrated during training.</p>
                        </div>

                        <!-- 7. Advanced Tests -->
                        <div class="methodology-section paper-section">
                            <h2>7. Advanced Statistical Tests</h2>

                            <h3>7.1 N-gram Perplexity Approximation</h3>
                            <p>We approximate neural language model perplexity using Markov chain entropy:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.1 (Perplexity)</p>
                                $$PP = 2^H \quad \text{where} \quad H = -\frac{1}{N}\sum_{i=1}^{N} \log_2 P(w_i | \text{context})$$
                            </div>
                            
                            <p><strong>Typical ranges:</strong> Human text: PP ≈ 100-300. AI text: PP ≈ 30-80 (more predictable).</p>

                            <h3>7.2 Runs Test for Randomness</h3>
                            <p>We test whether sequences (e.g., sentence lengths) show random or patterned behavior:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.2 (Runs Test)</p>
                                $$Z = \frac{R - E[R]}{\sigma_R} \quad \text{where} \quad E[R] = \frac{2n_0 n_1}{n} + 1$$
                            </div>
                            
                            <p>$R$ = observed runs, $n_0, n_1$ = counts above/below median. Fewer runs than expected indicates non-random structure.</p>

                            <h3>7.3 Chi-Squared Uniformity Test</h3>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.3 (Chi-Squared)</p>
                                $$\chi^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}$$
                            </div>
                            
                            <p>Tests whether observed distribution differs significantly from uniform. Higher $\chi^2$ = less uniform = more human-like.</p>

                            <h3>7.4 Mahalanobis Distance</h3>
                            <p>Multivariate distance from expected human feature distribution:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.4 (Mahalanobis Distance)</p>
                                $$D_M = \sqrt{\sum_{i=1}^{k} \left(\frac{x_i - \mu_i}{\sigma_i}\right)^2 / k}$$
                            </div>
                            
                            <p>Distance > 2σ indicates statistically unusual text (either extreme of the bell curve).</p>
                        </div>

                        <!-- 7.5 Advanced Detection Methods (V3 Features) -->
                        <div class="methodology-section paper-section">
                            <h2>7.5 Advanced Detection Methods (Sunrise V3)</h2>
                            
                            <p>Version 3 of the Sunrise model introduces advanced detection techniques covering 96 features across 12 categories. These methods address sophisticated AI text and humanizer evasion.</p>

                            <h3>7.5.1 Neural Embedding Analysis</h3>
                            <p>We use sentence-level embeddings to analyze semantic patterns:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.5 (Embedding Coherence)</p>
                                $$C_{emb} = \frac{1}{n-1}\sum_{i=1}^{n-1} \cos(\vec{e}_i, \vec{e}_{i+1})$$
                            </div>
                            
                            <p>where $\vec{e}_i$ is the embedding of sentence $i$. Higher coherence indicates smooth semantic flow.</p>
                            
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Metric</th><th>Description</th><th>Human</th><th>AI</th><th>Humanized</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Embedding Coherence</td><td>Adjacent sentence similarity</td><td>0.6-0.8</td><td>0.85+</td><td>0.5-0.7</td></tr>
                                        <tr><td>Embedding Diversity</td><td>Variance in embedding space</td><td>High</td><td>Low</td><td>Medium</td></tr>
                                        <tr><td>Embedding Drift</td><td>Topic shift over document</td><td>Gradual</td><td>Minimal</td><td>Erratic</td></tr>
                                        <tr><td>Embedding Clustering</td><td>Semantic grouping strength</td><td>Moderate</td><td>High</td><td>Low</td></tr>
                                        <tr><td>Embedding Uniformity</td><td>Consistency of distribution</td><td>Variable</td><td>Uniform</td><td>Artificially varied</td></tr>
                                    </tbody>
                                </table>
                            </div>

                            <h3>7.5.2 Perplexity Scoring</h3>
                            <p>Perplexity measures how "surprising" text is to a language model. AI text is typically less surprising:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.6 (Sentence Perplexity)</p>
                                $$PP(s) = \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log P(w_i | w_1, \ldots, w_{i-1})\right)$$
                            </div>
                            
                            <ul>
                                <li><strong>Perplexity Mean:</strong> Average perplexity across sentences. AI text: 20-60, Human text: 80-200</li>
                                <li><strong>Perplexity Std Dev:</strong> Variance in perplexity. Human text shows higher variance</li>
                                <li><strong>Perplexity Burstiness:</strong> Clustering of low/high perplexity regions</li>
                                <li><strong>Low Perplexity Ratio:</strong> Fraction of text with unusually low perplexity</li>
                            </ul>

                            <h3>7.5.3 Watermark Detection</h3>
                            <p>AI-generated text may contain statistical watermarks—subtle patterns inserted during generation:</p>
                            
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Detection Method</th><th>Description</th><th>Detection Signal</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>Zero-Width Characters</td><td>Invisible Unicode characters between words</td><td>Presence indicates watermarking</td></tr>
                                        <tr><td>Token Repetition Patterns</td><td>Unusual token transition frequencies</td><td>Non-random token selection bias</td></tr>
                                        <tr><td>Word Length Autocorrelation</td><td>Serial correlation in word lengths</td><td>Periodic patterns = watermark</td></tr>
                                        <tr><td>Green/Red Token Lists</td><td>Token probability manipulation</td><td>Statistical bias toward "green" tokens</td></tr>
                                    </tbody>
                                </table>
                            </div>

                            <h3>7.5.4 Semantic Coherence Analysis</h3>
                            <p>Beyond local coherence, we measure document-level semantic structure:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.7 (Topic Drift)</p>
                                $$D_{topic} = \frac{1}{k-1}\sum_{i=1}^{k-1} (1 - \cos(\vec{t}_i, \vec{t}_{i+1}))$$
                            </div>
                            
                            <ul>
                                <li><strong>Topic Drift:</strong> Rate of topic change across paragraphs. Human: gradual, AI: minimal, Humanized: erratic</li>
                                <li><strong>Coherence Variance:</strong> Consistency of semantic flow. Human: variable, AI: stable, Humanized: unstable</li>
                                <li><strong>Semantic Gaps:</strong> Sudden topic jumps. Human: intentional, AI: rare, Humanized: artificial</li>
                            </ul>

                            <h3>7.5.5 Null Combination Patterns</h3>
                            <p>A key V3 innovation: detecting feature <em>absence</em> patterns that strongly indicate specific classes:</p>
                            
                            <div class="equation-block">
                                <p class="equation-label">Definition 7.8 (Null Combination Score)</p>
                                $$NCS = f(A \land \neg B) = 
                                \begin{cases}
                                    1 & \text{if feature A present AND feature B absent} \\
                                    0 & \text{otherwise}
                                \end{cases}$$
                            </div>
                            
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Combination</th><th>Logic</th><th>Indicates</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>formal_no_contractions</td><td>Formal + No contractions</td><td>Pure AI</td></tr>
                                        <tr><td>ai_phrases_no_disfluencies</td><td>AI phrases + No disfluencies</td><td>Pure AI</td></tr>
                                        <tr><td>contractions_no_ai</td><td>Contractions + No AI phrases</td><td>Pure Human</td></tr>
                                        <tr><td>ai_with_contractions</td><td>AI phrases + Contractions present</td><td>Humanized</td></tr>
                                        <tr><td>formal_with_disfluencies</td><td>Formal style + Disfluencies</td><td>Humanized</td></tr>
                                    </tbody>
                                </table>
                            </div>

                            <h3>7.5.6 Partial Humanization Detection</h3>
                            <p>Detects text that is only partially humanized—common when users manually edit AI output:</p>
                            
                            <ul>
                                <li><strong>Humanization Variance:</strong> Inconsistency in humanization level across document</li>
                                <li><strong>Segment Inconsistency:</strong> Adjacent segments with different detected authorship</li>
                                <li><strong>AI Segment Ratio:</strong> Proportion of text segments classified as AI</li>
                                <li><strong>Human Segment Ratio:</strong> Proportion classified as human</li>
                                <li><strong>Mixed Segment Ratio:</strong> Proportion showing mixed signals</li>
                            </ul>
                            
                            <p>High segment inconsistency with varying AI/human ratios indicates selective editing of AI content.</p>
                        </div>

                        <!-- 8. Model Training & Performance -->
                        <div class="methodology-section paper-section">
                            <h2>8. Model Training & Performance</h2>
                            
                            <h3>8.1 Training Data</h3>
                            <p>All Veritas models were trained on a curated dataset of 29,976 text samples from diverse sources:</p>
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Dataset</th><th>Samples</th><th>Description</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td>GPT-Wiki-Intro</td><td>~30,000</td><td>AI-generated Wikipedia introductions paired with human originals (balanced 50/50)</td></tr>
                                        <tr><td>IMDB Reviews</td><td>~20,000</td><td>Human-written movie reviews with diverse emotional content and informal style</td></tr>
                                        <tr><td>OpenWebText</td><td>~20,000</td><td>Human web content from high-quality Reddit-linked sources</td></tr>
                                    </tbody>
                                </table>
                            </div>
                            <p>Final balanced dataset: 14,988 human samples + 14,988 AI samples = 29,976 total.</p>

                            <h3>8.2 Model Comparison</h3>
                            <div class="weight-table">
                                <table>
                                    <thead>
                                        <tr><th>Model</th><th>Accuracy</th><th>F1 Score</th><th>ROC-AUC</th><th>Features</th><th>Specialization</th></tr>
                                    </thead>
                                    <tbody>
                                        <tr><td><strong>Helios v4.0</strong></td><td>99.24%</td><td>99.24%</td><td>99.98%</td><td>45</td><td>Flagship: Tone, hedging, rhetorical analysis</td></tr>
                                        <tr><td><strong>Zenith v1.0</strong></td><td>99.57%</td><td>99.57%</td><td>99.97%</td><td>35</td><td>Perplexity-based, humanized text detection (86.7%)</td></tr>
                                        <tr><td><strong>Sunrise v3.0</strong></td><td>98.08%</td><td>98.09%</td><td>99.80%</td><td>37</td><td>Balanced general-purpose detection</td></tr>
                                        <tr><td><strong>Dawn v1.0</strong></td><td>84.9%</td><td>~85%</td><td>~92%</td><td>14</td><td>Legacy rule-based heuristics</td></tr>
                                    </tbody>
                                </table>
                            </div>

                            <h3>8.3 Helios Model v4.0 Features (45 total)</h3>
                            <ul>
                                <li><strong>Structural (6 features):</strong> Sentence/paragraph length, variance, uniformity, complexity</li>
                                <li><strong>Entropy (2 features):</strong> Bigram and trigram entropy (key discriminators)</li>
                                <li><strong>Lexical (4 features):</strong> Diversity, word length, function words, hapax ratio</li>
                                <li><strong>Tone (4 features):</strong> Formality score, emotional intensity, sentiment variance, consistency</li>
                                <li><strong>Hedging (3 features):</strong> Hedge word density, qualifiers, uncertainty markers</li>
                                <li><strong>Personal Voice (5 features):</strong> First/second person pronouns, opinion markers, anecdotes</li>
                                <li><strong>Rhetorical (4 features):</strong> Question density, argument balance, discourse markers, topic transitions</li>
                                <li><strong>Semantic (5 features):</strong> Coherence, topic drift, lexical chains, reference density</li>
                                <li><strong>Punctuation (4 features):</strong> Comma rhythm, variety, density patterns</li>
                                <li><strong>AI Signatures (8 features):</strong> AI phrases, formal patterns, mechanical transitions</li>
                            </ul>

                            <h3>8.4 Top Predictive Features (Helios Model)</h3>
                            <p>Feature importance analysis revealed the following as most predictive of AI authorship:</p>
                            <ol>
                                <li><strong>Paragraph uniformity</strong> (39.33% importance) — AI paragraphs are unnaturally consistent in length</li>
                                <li><strong>Trigram entropy</strong> (15.39% importance) — 3-word pattern predictability</li>
                                <li><strong>Bigram entropy</strong> (14.93% importance) — 2-word pattern diversity</li>
                                <li><strong>Average paragraph length</strong> (6.67% importance) — AI tends toward specific ranges</li>
                                <li><strong>Average sentence length</strong> (5.08% importance) — Sentence consistency patterns</li>
                                <li><strong>Lexical diversity</strong> (4.08% importance) — Vocabulary richness</li>
                                <li><strong>Comma rhythm variance</strong> (3.49% importance) — Punctuation pattern regularity</li>
                            </ol>
                        </div>

                        <!-- 9. Limitations -->
                        <div class="methodology-section paper-section">
                            <h2>9. Limitations & Responsible Use</h2>
                            
                            <h3>9.1 High-Risk Scenarios for False Positives</h3>
                            <ul>
                                <li><strong>Short texts (&lt;100 words):</strong> Insufficient statistical power for reliable detection—confidence is reduced</li>
                                <li><strong>Academic/technical writing:</strong> Formal styles with consistent structure may appear AI-like due to genre conventions</li>
                                <li><strong>Non-native English speakers:</strong> Second-language patterns may differ from native English baselines</li>
                                <li><strong>Heavily edited/revised text:</strong> Professional editing can smooth natural variance patterns</li>
                                <li><strong>Templated content:</strong> Business letters, legal documents with standard formats</li>
                            </ul>

                            <h3>9.2 Fundamental Constraints</h3>
                            <ul>
                                <li><strong>No ground truth:</strong> We cannot definitively know the true origin of any text</li>
                                <li><strong>Evolving AI:</strong> Detection methods must continuously adapt as language models improve</li>
                                <li><strong>Adversarial evasion:</strong> Determined adversaries with sufficient effort can defeat any detector</li>
                                <li><strong>Mixed authorship:</strong> Human-edited AI text or AI-assisted human writing creates ambiguity</li>
                            </ul>
                            
                            <div class="philosophy-box disclaimer">
                                <p><strong>Important:</strong> VERITAS provides probabilistic assessment, not definitive proof. Results should inform human judgment, not replace it. Never use AI detection as the sole basis for academic misconduct accusations or employment decisions.</p>
                            </div>
                        </div>

                        <!-- 10. Interpretation -->
                        <div class="methodology-section paper-section">
                            <h2>10. Interpretation Guide</h2>
                            
                            <div class="interpretation-grid">
                                <div class="interp-card ai">
                                    <h4><span class="ai-indicator"></span> AI Indicators</h4>
                                    <ul>
                                        <li>Sentence length CV &lt; 0.3</li>
                                        <li>Uniformity score &gt; 0.7</li>
                                        <li>Burstiness &lt; 0 (periodic)</li>
                                        <li>Zipf slope deviation &gt; 0.2</li>
                                        <li>Hapax ratio &lt; 0.4</li>
                                        <li>Decorative Unicode present</li>
                                        <li>High autocorrelation</li>
                                        <li>Stable local variance</li>
                                    </ul>
                                </div>
                                <div class="interp-card human">
                                    <h4><span class="human-indicator"></span> Human Indicators</h4>
                                    <ul>
                                        <li>Sentence length CV &gt; 0.5</li>
                                        <li>Positive burstiness (&gt; 0.2)</li>
                                        <li>Zipf compliance (slope ≈ -1)</li>
                                        <li>High hapax ratio (&gt; 0.5)</li>
                                        <li>Natural contractions</li>
                                        <li>Variable local variance</li>
                                        <li>Slow autocorrelation decay</li>
                                        <li>Style drift present</li>
                                    </ul>
                                </div>
                                <div class="interp-card humanized">
                                    <h4><span class="mixed-indicator"></span> Humanized AI Indicators</h4>
                                    <ul>
                                        <li>High variance but stable variance-of-variance</li>
                                        <li>Weak feature correlations</li>
                                        <li>Flat autocorrelation (random noise)</li>
                                        <li>Inconsistent word sophistication</li>
                                        <li>Micro-coherence disruptions</li>
                                        <li>Artificial contractions added</li>
                                        <li>Synonym-substitution artifacts</li>
                                    </ul>
                                </div>
                            </div>
                        </div>

                        <!-- 11. References -->
                        <div class="methodology-section paper-section">
                            <h2>11. References</h2>
                            <div class="references">
                                <p>[1] Zipf, G.K. (1949). <em>Human Behavior and the Principle of Least Effort</em>. Addison-Wesley.</p>
                                <p>[2] Shannon, C.E. (1948). A Mathematical Theory of Communication. <em>Bell System Technical Journal</em>, 27(3), 379-423.</p>
                                <p>[3] Yule, G.U. (1944). <em>The Statistical Study of Literary Vocabulary</em>. Cambridge University Press.</p>
                                <p>[4] Simpson, E.H. (1949). Measurement of Diversity. <em>Nature</em>, 163, 688.</p>
                                <p>[5] Goh, K.I. & Barabási, A.L. (2008). Burstiness and memory in complex systems. <em>EPL</em>, 81(4).</p>
                                <p>[6] Wilson, E.B. (1927). Probable inference, the law of succession, and statistical inference. <em>JASA</em>, 22(158), 209-212.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>
        </main>

        <!-- Footer -->
        <footer class="footer">
            <div class="footer-content">
                <div class="footer-main">
                    <div class="footer-brand">
                        <span class="footer-logo">◈</span>
                        <span class="footer-title">VERITAS</span>
                        <span class="footer-version"><span class="material-icons" style="font-size:14px;vertical-align:middle">wb_sunny</span> Powered by the Sun Suite v4.0</span>
                    </div>
                    <div class="footer-model-stats">
                        <span class="model-stat"><strong>Helios</strong> 99.24%</span>
                        <span class="model-stat-divider">•</span>
                        <span class="model-stat"><strong>Zenith</strong> 99.57%</span>
                        <span class="model-stat-divider">•</span>
                        <span class="model-stat"><strong>Sunrise</strong> 98.08%</span>
                        <span class="model-stat-divider">•</span>
                        <span class="model-stat"><strong>Dawn</strong> 84.9%</span>
                    </div>
                </div>
                <div class="footer-notes">
                    <span class="note-item">No single metric is definitive — we combine multiple signals</span>
                    <span class="note-divider">|</span>
                    <span class="note-item">Context matters — academic/technical writing may appear AI-like</span>
                    <span class="note-divider">|</span>
                    <span class="note-item">Confidence varies — short texts have higher uncertainty</span>
                    <span class="note-divider">|</span>
                    <span class="note-item">AI evolves — detection methods must continuously adapt</span>
                </div>
            </div>
        </footer>
    </div>

    <!-- External Libraries -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jszip/3.10.1/jszip.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>
    <script src="https://unpkg.com/docx@8.2.4/build/index.umd.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
    
    <!-- Core Utilities -->
    <script src="js/utils.js"></script>
    <script src="js/variance-utils.js"></script>
    <script src="js/sunrise-model.js"></script>
    <script src="js/zenith-model.js"></script>
    <script src="js/helios-model.js"></script>
    
    <!-- Analyzers -->
    <script src="js/analyzers/grammar.js"></script>
    <script src="js/analyzers/syntax.js"></script>
    <script src="js/analyzers/lexical.js"></script>
    <script src="js/analyzers/dialect.js"></script>
    <script src="js/analyzers/archaic.js"></script>
    <script src="js/analyzers/discourse.js"></script>
    <script src="js/analyzers/semantic.js"></script>
    <script src="js/analyzers/statistical.js"></script>
    <script src="js/analyzers/authorship.js"></script>
    <script src="js/analyzers/metapatterns.js"></script>
    <script src="js/analyzers/metadata.js"></script>
    <script src="js/analyzers/repetition.js"></script>
    <script src="js/analyzers/tone.js"></script>
    <script src="js/analyzers/partofspeech.js"></script>
    <script src="js/analyzers/humanization.js"></script>
    
    <!-- Engine & Features -->
    <script src="js/analyzer-engine.js"></script>
    <script src="js/visualizations.js"></script>
    <script src="js/advanced-visualizations.js"></script>
    <script src="js/file-parser.js"></script>
    <script src="js/report-exporter.js"></script>
    <script src="js/humanizer.js"></script>
    <script src="js/app.js"></script>
    
    <!-- KaTeX Auto-render for methodology page -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            if (typeof renderMathInElement !== 'undefined') {
                // Render on initial load if methodology view is visible
                const methodologyView = document.getElementById('methodologyView');
                if (methodologyView) {
                    renderMathInElement(methodologyView, {
                        delimiters: [
                            {left: '$$', right: '$$', display: true},
                            {left: '$', right: '$', display: false}
                        ],
                        throwOnError: false
                    });
                }
                
                // Re-render when methodology tab is clicked
                document.querySelectorAll('[data-view="methodology"]').forEach(link => {
                    link.addEventListener('click', function() {
                        setTimeout(() => {
                            if (typeof renderMathInElement !== 'undefined') {
                                renderMathInElement(document.getElementById('methodologyView'), {
                                    delimiters: [
                                        {left: '$$', right: '$$', display: true},
                                        {left: '$', right: '$', display: false}
                                    ],
                                    throwOnError: false
                                });
                            }
                        }, 100);
                    });
                });
            }
        });
    </script>
</body>
</html>
